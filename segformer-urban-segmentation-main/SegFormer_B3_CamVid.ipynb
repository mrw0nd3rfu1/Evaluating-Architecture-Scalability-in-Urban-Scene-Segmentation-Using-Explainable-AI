{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89680cd-17f9-4ea4-a9f2-ebe7f0971061",
   "metadata": {},
   "source": [
    "# **Initial Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff3df19-50e1-411c-9320-bf0cdef29773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 1\n",
      "GPU Device Name: Tesla V100-SXM2-32GB\n",
      "Current GPU Device: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def check_gpu():\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"GPU Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Current GPU Device: {torch.cuda.current_device()}\")\n",
    "    else:\n",
    "        print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "check_gpu()\n",
    "\n",
    "# CUDA configs\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654f51da-4bb0-46a4-b70f-bff879d0fdcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: opencv-python in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcc12/opencv/4.11.0/lib/python3.11/site-packages (4.11.0)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: numpy in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (2.2.2+computecanada)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: pandas in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (2.2.3+computecanada)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from pandas) (2.2.2+computecanada)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from pandas) (2.9.0.post0+computecanada)\n",
      "Requirement already satisfied: pytz>=2020.1 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from pandas) (2025.1+computecanada)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from pandas) (2025.1+computecanada)\n",
      "Requirement already satisfied: six>=1.5 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0+computecanada)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: torch in /home/thatkar/.local/lib/python3.11/site-packages (2.6.0+computecanada)\n",
      "Requirement already satisfied: torchvision in /home/thatkar/.local/lib/python3.11/site-packages (0.21.0+computecanada)\n",
      "Requirement already satisfied: filelock in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/ipykernel/2025a/lib/python3.11/site-packages (from torch) (4.12.2+computecanada)\n",
      "Requirement already satisfied: networkx in /home/thatkar/.local/lib/python3.11/site-packages (from torch) (3.4.2+computecanada)\n",
      "Requirement already satisfied: jinja2 in /home/thatkar/.local/lib/python3.11/site-packages (from torch) (3.1.5+computecanada)\n",
      "Requirement already satisfied: fsspec in /home/thatkar/.local/lib/python3.11/site-packages (from torch) (2025.2.0+computecanada)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/thatkar/.local/lib/python3.11/site-packages (from torch) (1.13.1+computecanada)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0+computecanada)\n",
      "Requirement already satisfied: numpy in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from torchvision) (2.2.2+computecanada)\n",
      "Requirement already satisfied: pillow-simd!=8.3.*,>=5.3.0 in /home/thatkar/.local/lib/python3.11/site-packages (from torchvision) (9.5.0.post2+computecanada)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thatkar/.local/lib/python3.11/site-packages (from jinja2->torch) (2.1.5+computecanada)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/bin/pip\", line 5, in <module>\n",
      "    from pip._internal.cli.main import main\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
      "    from pip._internal.build_env import get_runnable_pip\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
      "    from pip._internal.cli.spinners import open_spinner\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
      "    from pip._internal.utils.logging import get_indentation\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n",
      "    from pip._internal.utils.misc import ensure_dir\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_internal/utils/misc.py\", line 40, in <module>\n",
      "    from pip._internal.exceptions import CommandError, ExternallyManagedEnvironment\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_internal/exceptions.py\", line 18, in <module>\n",
      "    from pip._vendor.requests.models import Request, Response\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_vendor/requests/__init__.py\", line 45, in <module>\n",
      "    from .exceptions import RequestsDependencyWarning\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_vendor/requests/exceptions.py\", line 9, in <module>\n",
      "    from .compat import JSONDecodeError as CompatJSONDecodeError\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_vendor/requests/compat.py\", line 10, in <module>\n",
      "    from pip._vendor import chardet\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_vendor/chardet/__init__.py\", line 24, in <module>\n",
      "    from .universaldetector import UniversalDetector\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_vendor/chardet/universaldetector.py\", line 50, in <module>\n",
      "    from .mbcsgroupprober import MBCSGroupProber\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_vendor/chardet/mbcsgroupprober.py\", line 30, in <module>\n",
      "    from .big5prober import Big5Prober\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_vendor/chardet/big5prober.py\", line 28, in <module>\n",
      "    from .chardistribution import Big5DistributionAnalysis\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/pip/_vendor/chardet/chardistribution.py\", line 55, in <module>\n",
      "    from .johabfreq import JOHAB_TO_EUCKR_ORDER_TABLE\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: matplotlib in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (3.10.0+computecanada)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (1.3.1+computecanada)\n",
      "Requirement already satisfied: cycler>=0.10 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (0.12.1+computecanada)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (4.55.8+computecanada)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (1.4.8+computecanada)\n",
      "Requirement already satisfied: numpy>=1.23 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (2.2.2+computecanada)\n",
      "Requirement already satisfied: packaging>=20.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (24.2+computecanada)\n",
      "Requirement already satisfied: pillow>=8 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (11.1.0+computecanada)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (3.2.1+computecanada)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0+computecanada)\n",
      "Requirement already satisfied: six>=1.5 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0+computecanada)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: albumentations in /home/thatkar/.local/lib/python3.11/site-packages (2.0.4)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from albumentations) (2.2.2+computecanada)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from albumentations) (1.15.1+computecanada)\n",
      "Requirement already satisfied: PyYAML in /home/thatkar/.local/lib/python3.11/site-packages (from albumentations) (6.0.2+computecanada)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /home/thatkar/.local/lib/python3.11/site-packages (from albumentations) (2.10.6+computecanada)\n",
      "Requirement already satisfied: albucore==0.0.23 in /home/thatkar/.local/lib/python3.11/site-packages (from albumentations) (0.0.23+computecanada)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcc12/opencv/4.11.0/lib/python3.11/site-packages (from albumentations) (4.11.0)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /home/thatkar/.local/lib/python3.11/site-packages (from albucore==0.0.23->albumentations) (3.10.5+computecanada)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /home/thatkar/.local/lib/python3.11/site-packages (from albucore==0.0.23->albumentations) (6.0.5+computecanada)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/thatkar/.local/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0+computecanada)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/thatkar/.local/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (2.27.2+computecanada)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/ipykernel/2025a/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (4.12.2+computecanada)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: jupyterlab-widgets in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/ipykernel/2025a/lib/python3.11/site-packages (3.0.13+computecanada)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install ipywidgets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install jupyterlab-widgets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TqdmWarning\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install torch torchvision\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install albumentations\n",
    "!pip install ipywidgets\n",
    "!pip install jupyterlab-widgets\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import TqdmWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa4056-dffe-4c2f-b330-54868905af37",
   "metadata": {},
   "source": [
    "# **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edddd07e-9079-46f7-82fc-0534f7ebaa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thatkar/.local/lib/python3.11/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = \"/home/thatkar/projects/def-saadi/thatkar\"\n",
    "ROOT_DIR = os.path.join(BASE_DIR, \"CamVid\")\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, \"train\")\n",
    "TRAIN_LABELS_DIR = os.path.join(ROOT_DIR, \"train_labels\")\n",
    "VAL_DIR = os.path.join(ROOT_DIR, \"val\")\n",
    "VAL_LABELS_DIR = os.path.join(ROOT_DIR, \"val_labels\")\n",
    "TEST_DIR = os.path.join(ROOT_DIR, \"test\")\n",
    "TEST_LABELS_DIR = os.path.join(ROOT_DIR, \"test_labels\")\n",
    "CLASS_DICT_PATH = os.path.join(ROOT_DIR, \"class_dict.csv\")\n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, \"checkpoints\")\n",
    "VISUALIZATION_DIR = os.path.join(ROOT_DIR, \"visualizations\")\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [CHECKPOINT_DIR, VISUALIZATION_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Training Hyperparameters (kept same as B5 for fair comparison)\n",
    "IMAGE_HEIGHT = 960\n",
    "IMAGE_WIDTH = 768\n",
    "BATCH_SIZE = 1\n",
    "ACCUMULATION_STEPS = 16\n",
    "NUM_EPOCHS = 45\n",
    "LEARNING_RATE = 2e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP_VALUE = 1.0\n",
    "\n",
    "# Define data augmentation pipelines\n",
    "train_transforms = A.Compose([\n",
    "    A.RandomResizedCrop(\n",
    "        size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        scale=(0.8, 1.0),\n",
    "        ratio=(0.75, 1.33),\n",
    "        p=1.0,\n",
    "        interpolation=cv2.INTER_LINEAR\n",
    "    ),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2,\n",
    "            contrast_limit=0.2,\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "        A.RandomShadow(\n",
    "            shadow_roi=(0, 0.5, 1, 1),\n",
    "            p=0.5\n",
    "        ),\n",
    "    ], p=0.7),\n",
    "    A.OneOf([\n",
    "        A.Affine(\n",
    "            scale=(0.9, 1.1),\n",
    "            translate_percent={'x': (-0.1, 0.1), 'y': (-0.1, 0.1)},\n",
    "            rotate=(-15, 15),\n",
    "            border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        A.ElasticTransform(\n",
    "            alpha=120,\n",
    "            sigma=6,\n",
    "            p=0.5\n",
    "        ),\n",
    "    ], p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for CamVid with enhanced validation and error handling\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, label_dir, feature_extractor, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.ignore_index = 255\n",
    "        \n",
    "        # Load class definitions and create mappings\n",
    "        self.class_df = pd.read_csv(CLASS_DICT_PATH)\n",
    "        self.num_classes = len(self.class_df)\n",
    "        \n",
    "        # Create color mapping dictionary (BGR format for OpenCV)\n",
    "        self.color_mapping = {}\n",
    "        for idx, row in self.class_df.iterrows():\n",
    "            bgr_color = (int(row['b']), int(row['g']), int(row['r']))\n",
    "            self.color_mapping[bgr_color] = idx\n",
    "        \n",
    "        # Validate and filter images\n",
    "        all_images = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.images = []\n",
    "        \n",
    "        print(f\"Validating images in {image_dir}...\")\n",
    "        for img_name in all_images:\n",
    "            label_name = img_name.replace('.png', '_L.png')\n",
    "            img_path = os.path.join(image_dir, img_name)\n",
    "            label_path = os.path.join(label_dir, label_name)\n",
    "            \n",
    "            if os.path.exists(label_path):\n",
    "                img_test = cv2.imread(img_path)\n",
    "                label_test = cv2.imread(label_path)\n",
    "                if img_test is not None and label_test is not None:\n",
    "                    self.images.append(img_name)\n",
    "                else:\n",
    "                    print(f\"Warning: Failed to load {img_name} or its label\")\n",
    "            else:\n",
    "                print(f\"Warning: Missing label for {img_name}\")\n",
    "        \n",
    "        print(f\"Found {len(self.images)} valid image-label pairs\")\n",
    "        \n",
    "        if len(self.images) == 0:\n",
    "            raise RuntimeError(f\"No valid image-label pairs found in {image_dir}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load and process input image\n",
    "            image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load and process label image\n",
    "            label_path = os.path.join(self.label_dir, self.images[idx].replace('.png', '_L.png'))\n",
    "            label = cv2.imread(label_path)\n",
    "            if label is None:\n",
    "                raise ValueError(f\"Failed to load label: {label_path}\")\n",
    "            \n",
    "            # Create label mask using vectorized operations\n",
    "            h, w = label.shape[:2]\n",
    "            pixels = label.reshape(-1, 3)\n",
    "            pixel_classes = np.full(len(pixels), self.ignore_index, dtype=np.int64)\n",
    "            \n",
    "            for bgr_color, class_idx in self.color_mapping.items():\n",
    "                matches = np.all(pixels == bgr_color, axis=1)\n",
    "                pixel_classes[matches] = class_idx\n",
    "            label_mask = pixel_classes.reshape(h, w)\n",
    "            \n",
    "            # Apply transforms with error handling\n",
    "            if self.transforms:\n",
    "                try:\n",
    "                    transformed = self.transforms(image=image, mask=label_mask)\n",
    "                    image = transformed['image']\n",
    "                    label_mask = transformed['mask']\n",
    "                except Exception as e:\n",
    "                    print(f\"Transform error for image {self.images[idx]}: {str(e)}\")\n",
    "                    raise\n",
    "                    \n",
    "            return {\n",
    "                'pixel_values': image,\n",
    "                'labels': torch.as_tensor(label_mask, dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {self.images[idx]}: {str(e)}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "class EnhancedSegmentationLoss(nn.Module):\n",
    "    def __init__(self, num_classes, ignore_index=255):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "        # Enhanced class weights\n",
    "        class_weights = torch.ones(num_classes)\n",
    "        rare_classes = [6, 9, 10, 15, 16, 23, 24]\n",
    "        small_objects = [6, 8, 23, 24]\n",
    "        vehicle_classes = [5, 22, 27]\n",
    "        dark_objects = [5, 22, 27, 8, 23]\n",
    "        \n",
    "        for class_idx in range(num_classes):\n",
    "            if class_idx in dark_objects:\n",
    "                class_weights[class_idx] = 3.5\n",
    "            elif class_idx in small_objects:\n",
    "                class_weights[class_idx] = 3.0\n",
    "            elif class_idx in vehicle_classes:\n",
    "                class_weights[class_idx] = 2.5\n",
    "            elif class_idx in rare_classes:\n",
    "                class_weights[class_idx] = 2.0\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights, ignore_index=ignore_index)\n",
    "        self.smooth = 1e-5\n",
    "\n",
    "    def get_boundaries(self, tensor):\n",
    "        boundaries = torch.zeros_like(tensor, dtype=torch.float)\n",
    "        kernel_sizes = [3, 5, 7, 9]\n",
    "        weights = [0.4, 0.3, 0.2, 0.1]\n",
    "        \n",
    "        for k_size, weight in zip(kernel_sizes, weights):\n",
    "            pooled = F.max_pool2d(\n",
    "                tensor.float(),\n",
    "                kernel_size=k_size,\n",
    "                stride=1,\n",
    "                padding=k_size//2\n",
    "            )\n",
    "            boundaries += weight * (pooled != tensor.float()).float()\n",
    "        return boundaries\n",
    "\n",
    "    def calculate_iou_loss(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        pred = pred.flatten(2)\n",
    "        target = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2).flatten(2)\n",
    "        \n",
    "        intersection = (pred * target).sum(-1)\n",
    "        total = (pred + target).sum(-1)\n",
    "        union = total - intersection\n",
    "        valid_mask = union > self.smooth\n",
    "        iou = torch.zeros_like(intersection)\n",
    "        iou[valid_mask] = (intersection[valid_mask] + self.smooth) / (union[valid_mask] + self.smooth)\n",
    "        \n",
    "        return 1 - iou.mean()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        ce_loss = self.ce_loss(outputs, targets)\n",
    "        iou_loss = self.calculate_iou_loss(outputs, targets)\n",
    "        \n",
    "        edges = self.get_boundaries(targets)\n",
    "        pred_edges = self.get_boundaries(torch.argmax(outputs, dim=1))\n",
    "        boundary_loss = F.mse_loss(pred_edges, edges)\n",
    "        \n",
    "        total_loss = ce_loss + 0.4 * iou_loss + 0.8 * boundary_loss\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, scaler, criterion, device, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        try:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.amp.autocast(device_type=str(device), dtype=torch.float16):\n",
    "                outputs = model(pixel_values=pixel_values)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                logits = F.interpolate(\n",
    "                    logits,\n",
    "                    size=labels.shape[-2:],\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False\n",
    "                )\n",
    "                \n",
    "                loss = criterion(logits, labels) / ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VALUE)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "            \n",
    "            current_loss = loss.item() * ACCUMULATION_STEPS\n",
    "            epoch_loss += current_loss\n",
    "            batch_losses.append(current_loss)\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': current_loss,\n",
    "                'avg_loss': epoch_loss / (batch_idx + 1)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return epoch_loss / len(train_loader), batch_losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc='Validation'):\n",
    "        try:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            logits = F.interpolate(\n",
    "                logits,\n",
    "                size=labels.shape[-2:],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predictions.append(torch.argmax(logits, dim=1).cpu())\n",
    "            ground_truths.append(labels.cpu())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during validation: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return val_loss / len(val_loader), predictions, ground_truths\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, save_path):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
    "    plt.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
    "    \n",
    "    window_size = 5\n",
    "    if len(train_losses) >= window_size:\n",
    "        train_ma = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        val_ma = np.convolve(val_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(range(window_size-1, len(train_losses)), train_ma, \n",
    "                '--', color='darkblue', alpha=0.5, label='Train Moving Avg')\n",
    "        plt.plot(range(window_size-1, len(val_losses)), val_ma, \n",
    "                '--', color='darkred', alpha=0.5, label='Val Moving Avg')\n",
    "    \n",
    "    plt.title('Training and Validation Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Clear CUDA cache before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize feature extractor\n",
    "        feature_extractor = SegformerImageProcessor.from_pretrained(\n",
    "            \"nvidia/mit-b3\",  # Changed to B3\n",
    "            do_reduce_labels=True,\n",
    "            do_rescale=False,\n",
    "            size={\"height\": IMAGE_HEIGHT, \"width\": IMAGE_WIDTH}\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = CamVidDataset(TRAIN_DIR, TRAIN_LABELS_DIR, feature_extractor, transforms=train_transforms)\n",
    "        val_dataset = CamVidDataset(VAL_DIR, VAL_LABELS_DIR, feature_extractor, transforms=val_transforms)\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/mit-b3\",  # Changed to B3\n",
    "            num_labels=train_dataset.num_classes,\n",
    "            id2label={str(i): str(i) for i in range(train_dataset.num_classes)},\n",
    "            label2id={str(i): i for i in range(train_dataset.num_classes)},\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Enable memory efficient attention if available\n",
    "        if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "            model.config.use_memory_efficient_attention = True\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        total_steps = len(train_loader) * NUM_EPOCHS // ACCUMULATION_STEPS\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=LEARNING_RATE,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "        \n",
    "        criterion = EnhancedSegmentationLoss(train_dataset.num_classes).to(device)\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss, batch_losses = train_epoch(model, train_loader, optimizer, scheduler, scaler, criterion, device, epoch)\n",
    "            train_losses.append(train_loss)\n",
    "    \n",
    "            # Validation\n",
    "            val_loss, predictions, ground_truths = validate(model, val_loader, criterion, device)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, f'best_model_loss_b3_{val_loss:.4f}.pth'))\n",
    "                print(f\"New best model saved! Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Plot training curves\n",
    "            plot_training_curves(\n",
    "                train_losses,\n",
    "                val_losses,\n",
    "                os.path.join(VISUALIZATION_DIR, f'training_curves_epoch_{epoch+1}.png')\n",
    "            )\n",
    "            \n",
    "            # Clear cache after each epoch\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef0dec-40e7-45a7-9c40-764cfa76d69a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be7a23e-cac0-4f91-ad5d-2b3462b332af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /home/thatkar/projects/def-saadi/thatkar/CamVid/checkpoints/best_model_loss_b3_0.6197.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thatkar/.local/lib/python3.11/site-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Find the best model checkpoint\n",
    "checkpoint_dir = \"/home/thatkar/projects/def-saadi/thatkar/CamVid/checkpoints/\"\n",
    "best_model_path = \"/home/thatkar/projects/def-saadi/thatkar/CamVid/checkpoints/best_model_loss_b3_0.6197.pth\"\n",
    "print(f\"Loading checkpoint from: {best_model_path}\")\n",
    "\n",
    "TEST_IMAGE_PATH = \"/home/thatkar/projects/def-saadi/thatkar/CamVid/test/0001TP_006690.png\"\n",
    "# TEST_IMAGE_PATH = \"/home/thatkar/projects/def-saadi/thatkar/CamVid/test/0016E5_04530.png\"\n",
    "CLASS_DICT_PATH = \"/home/thatkar/projects/def-saadi/thatkar/CamVid/class_dict.csv\"\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load class definitions\n",
    "class_df = pd.read_csv(CLASS_DICT_PATH)\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = SegformerImageProcessor.from_pretrained(\n",
    "    \"nvidia/mit-b3\",  # Changed to B3\n",
    "    do_reduce_labels=True,\n",
    "    do_rescale=False,\n",
    "    size={\"height\": 640, \"width\": 640}\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",  # Changed to B3\n",
    "    num_labels=len(class_df),\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Get label path\n",
    "LABEL_PATH = TEST_IMAGE_PATH.replace('/test/', '/test_labels/').replace('.png', '_L.png')\n",
    "\n",
    "def visualize_prediction(model, image_path, label_path, feature_extractor, class_df, device):\n",
    "    # Read images\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    label = cv2.imread(label_path)\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(height=640, width=640),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    transformed = test_transform(image=image)\n",
    "    image_tensor = transformed['image'].unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=image_tensor.to(device))\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=(orig_h, orig_w),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        predicted = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Create visualization masks\n",
    "    pred_mask = np.zeros_like(image)\n",
    "    truth_mask = np.zeros_like(image)\n",
    "\n",
    "    for idx, row in class_df.iterrows():\n",
    "        class_color = np.array([row['r'], row['g'], row['b']])\n",
    "        pred_mask[predicted == idx] = class_color\n",
    "        bgr_color = (int(row['b']), int(row['g']), int(row['r']))\n",
    "        truth_pixels = np.all(label == bgr_color, axis=2)\n",
    "        truth_mask[truth_pixels] = class_color\n",
    "\n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(truth_mask)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(pred_mask)\n",
    "    axes[2].set_title('Model Prediction (B3)')  # Added B3 to title\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run visualization\n",
    "# visualize_prediction(model, TEST_IMAGE_PATH, LABEL_PATH, feature_extractor, class_df, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf78f3f-d4a3-4c76-9218-66b2dbba0823",
   "metadata": {},
   "source": [
    "# Testing Model Interpretability using XAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29feb51c-55a2-4c2b-ba58-1e8ee35b9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "\n",
    "def visualize_attention(model, image_tensor, processor, original_image, class_names, device):\n",
    "    \"\"\"\n",
    "    Generate and visualize the attention maps from the last layer for interpretability.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass with hooks\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=image_tensor)\n",
    "        logits = outputs.logits  # shape: [1, num_classes, H, W]\n",
    "        preds = torch.argmax(logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Upsample for original image size\n",
    "    upsampled = F.interpolate(logits, size=(IMAGE_HEIGHT, IMAGE_WIDTH), mode='bilinear', align_corners=False)\n",
    "    predicted_class = torch.argmax(upsampled, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axs[0].imshow(original_image)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(predicted_class, cmap='tab20')\n",
    "    axs[1].set_title(\"Predicted Segmentation\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b501c783-837e-450e-b013-cf7ce6f56a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thatkar/.local/lib/python3.11/site-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TEST_IMAGE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m processor \u001b[38;5;241m=\u001b[39m SegformerImageProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia/mit-b3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     do_reduce_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     do_rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     size\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: IMAGE_HEIGHT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: IMAGE_WIDTH}\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load one test image for explainability\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sample_img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mTEST_IMAGE_PATH\u001b[49m)  \u001b[38;5;66;03m# update this filename if needed\u001b[39;00m\n\u001b[1;32m     12\u001b[0m original \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(sample_img_path)\n\u001b[1;32m     13\u001b[0m original_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(original, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEST_IMAGE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "processor = SegformerImageProcessor.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    do_reduce_labels=True,\n",
    "    do_rescale=False,\n",
    "    size={\"height\": IMAGE_HEIGHT, \"width\": IMAGE_WIDTH}\n",
    ")\n",
    "\n",
    "# Load one test image for explainability\n",
    "sample_img_path = os.path.join(TEST_IMAGE_PATH)  # update this filename if needed\n",
    "original = cv2.imread(sample_img_path)\n",
    "original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "resized = cv2.resize(original_rgb, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "\n",
    "# Normalize like training\n",
    "normalized = processor(resized, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "\n",
    "# Load the best trained model\n",
    "model_path = os.path.join(CHECKPOINT_DIR, \"best_model_loss_b3_0.6197.pth\")  # Use the actual best file name\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Class names from CSV\n",
    "class_names = list(pd.read_csv(CLASS_DICT_PATH)['name'])\n",
    "\n",
    "# Run visualization\n",
    "visualize_attention(model, normalized, processor, resized, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1191781d-d3bf-4c94-8405-6c4860a00857",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchcam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradCAM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchcam'"
     ]
    }
   ],
   "source": [
    "from torchcam.methods import GradCAM\n",
    "from torchvision.transforms.functional import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    num_labels=32,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, \"best_model_loss_b3_0.6197.pth\"))[\"model_state_dict\"])\n",
    "model.eval().to(device)\n",
    "\n",
    "# Initialize Grad-CAM with last encoder layer\n",
    "cam_extractor = GradCAM(model, target_layer=\"encoder.encoder.layer.11\")  # Adjust layer depending on variant\n",
    "\n",
    "# Pick a sample image from test set\n",
    "sample_img_path = os.path.join(TEST_IMAGE_PATH)  # update this filename if needed\n",
    "img = cv2.imread(sample_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_resized = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "input_tensor = val_transforms(image=img_resized)[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "# Forward pass to get logits\n",
    "with torch.no_grad():\n",
    "    out = model(pixel_values=input_tensor)\n",
    "    logits = out.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Generate CAM for a specific class (e.g., class 10: car)\n",
    "target_class = 10\n",
    "activation_map = cam_extractor(class_idx=target_class, scores=logits)[0].cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_resized)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(preds.squeeze().cpu(), cmap=\"tab20\")\n",
    "plt.title(\"Predicted Segmentation\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_resized)\n",
    "plt.imshow(activation_map, cmap=\"jet\", alpha=0.5)\n",
    "plt.title(f\"Grad-CAM for class {target_class}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44be81-a5dd-4f44-a210-32b806cc040f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
