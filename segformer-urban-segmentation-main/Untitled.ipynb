{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283d4ede-1a9c-445c-b24e-4495e09d7bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Generating KITTI visualizations...\n",
      "\n",
      "Generating IDD visualizations...\n",
      "\n",
      "Visualization generation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Set font sizes for publication-quality figures\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16\n",
    "})\n",
    "\n",
    "# Configure paths\n",
    "BASE_DIR = \"/project/6087070/thatkar\"\n",
    "\n",
    "# KITTI settings\n",
    "KITTI_ROOT = os.path.join(BASE_DIR, \"KITTI\")\n",
    "KITTI_VAL_DIR = os.path.join(KITTI_ROOT, \"validation/image_2\")\n",
    "KITTI_VAL_LABELS_DIR = os.path.join(KITTI_ROOT, \"validation/semantic\")\n",
    "KITTI_BASELINE_CHECKPOINT = os.path.join(KITTI_ROOT, \"checkpoints/baseline/best_model_miou_0.5117.pth\")\n",
    "KITTI_TRANSFER_CHECKPOINT = os.path.join(KITTI_ROOT, \"checkpoints/transfer/best_model_miou_0.5342.pth\")\n",
    "KITTI_OUTPUT_DIR = os.path.join(KITTI_ROOT, \"journal_visualizations\")\n",
    "\n",
    "# IDD settings\n",
    "IDD_ROOT = os.path.join(BASE_DIR, \"IDD\")\n",
    "IDD_VAL_DIR = os.path.join(IDD_ROOT, \"leftImg8bit/val\")\n",
    "IDD_VAL_LABELS_DIR = os.path.join(IDD_ROOT, \"IDD_pixelwise_masks/val\")\n",
    "IDD_BASELINE_CHECKPOINT = os.path.join(IDD_ROOT, \"checkpoints/segformer_b3_idd/best_model_miou_0.5028.pth\")\n",
    "IDD_TRANSFER_CHECKPOINT = os.path.join(IDD_ROOT, \"checkpoints/segformer_b3_idd_transfer/best_model_miou_0.5108.pth\")\n",
    "IDD_OUTPUT_DIR = os.path.join(IDD_ROOT, \"journal_visualizations\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(KITTI_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(IDD_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define dataset classes\n",
    "# Import your dataset classes or define them here\n",
    "# For demonstration purposes, simplified dataset class:\n",
    "class SegmentationDataset:\n",
    "    def __init__(self, image_dir, label_dir, class_mapping, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.class_mapping = class_mapping\n",
    "        self.num_classes = len(class_mapping)\n",
    "        self.ignore_index = 255\n",
    "        \n",
    "        # Find valid image-label pairs\n",
    "        self.images = self._get_image_label_pairs()\n",
    "        \n",
    "    def _get_image_label_pairs(self):\n",
    "        # This should be implemented according to your dataset structure\n",
    "        # For demonstration, assume a simple structure\n",
    "        # In reality, you'll need to adapt this for KITTI and IDD\n",
    "        pairs = []\n",
    "        return pairs\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # This should be implemented according to your dataset structure\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def get_class_names(self):\n",
    "        \"\"\"Return a list of class names in order.\"\"\"\n",
    "        return [self.class_mapping[i]['name'] for i in range(self.num_classes)]\n",
    "    \n",
    "    def get_color_map(self):\n",
    "        \"\"\"Return a mapping of class IDs to colors for visualization.\"\"\"\n",
    "        return {i: self.class_mapping[i]['color'] for i in range(self.num_classes)}\n",
    "\n",
    "# Define utility functions\n",
    "def denormalize_image(img_tensor):\n",
    "    \"\"\"Convert normalized tensor back to displayable image.\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    img_tensor = img_tensor * std + mean\n",
    "    img = img_tensor.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def create_colored_mask(mask, class_colors):\n",
    "    \"\"\"Create a colored segmentation mask.\"\"\"\n",
    "    h, w = mask.shape\n",
    "    colored_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_id, color in class_colors.items():\n",
    "        class_mask = (mask == class_id)\n",
    "        if np.any(class_mask):\n",
    "            for c in range(3):\n",
    "                colored_mask[:, :, c][class_mask] = color[c]\n",
    "    \n",
    "    # Handle ignore index\n",
    "    ignore_mask = (mask == 255)\n",
    "    if np.any(ignore_mask):\n",
    "        colored_mask[ignore_mask] = [180, 180, 180]\n",
    "    \n",
    "    return colored_mask\n",
    "\n",
    "def calculate_metrics(pred_mask, gt_mask, num_classes, ignore_index=255):\n",
    "    \"\"\"Calculate IoU metrics for a single image.\"\"\"\n",
    "    class_ious = []\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_mask == c)\n",
    "        gt_c = (gt_mask == c)\n",
    "        \n",
    "        # Only consider valid pixels (not ignore_index)\n",
    "        valid_mask = (gt_mask != ignore_index)\n",
    "        pred_c = pred_c & valid_mask\n",
    "        gt_c = gt_c & valid_mask\n",
    "        \n",
    "        intersection = np.logical_and(pred_c, gt_c).sum()\n",
    "        union = np.logical_or(pred_c, gt_c).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = float('nan')  # Class not present\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "            \n",
    "        class_ious.append(iou)\n",
    "    \n",
    "    valid_ious = [iou for iou in class_ious if not np.isnan(iou)]\n",
    "    mean_iou = np.mean(valid_ious) if valid_ious else 0\n",
    "    \n",
    "    return {\n",
    "        'class_ious': class_ious,\n",
    "        'mean_iou': mean_iou\n",
    "    }\n",
    "\n",
    "def generate_journal_visualizations(dataset_name, val_dataset, baseline_model, transfer_model, \n",
    "                                   output_dir, device, num_samples=5, seed=42):\n",
    "    \"\"\"Generate standardized visualizations for journal publication.\"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Create subdirectories\n",
    "    comparison_dir = os.path.join(output_dir, \"comparisons\")\n",
    "    metrics_dir = os.path.join(output_dir, \"metrics\")\n",
    "    \n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "    \n",
    "    # Get class colors and names\n",
    "    class_colors = val_dataset.get_color_map()\n",
    "    class_names = val_dataset.get_class_names()\n",
    "    \n",
    "    # Create dataloader with fixed seed for reproducibility\n",
    "    dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=1, \n",
    "        shuffle=True, \n",
    "        num_workers=2, \n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    \n",
    "    # Process samples\n",
    "    all_baseline_metrics = []\n",
    "    all_transfer_metrics = []\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=f\"Generating {dataset_name} visualizations\")):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        gt_labels = batch['labels'].cpu().numpy()[0]\n",
    "        image_name = batch['image_name'][0]\n",
    "        \n",
    "        # Get original image\n",
    "        original_img = denormalize_image(pixel_values[0].cpu())\n",
    "        \n",
    "        # Get baseline model prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = baseline_model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            logits = F.interpolate(logits, size=gt_labels.shape, mode=\"bilinear\", align_corners=False)\n",
    "            baseline_pred = torch.argmax(logits, dim=1)[0].cpu().numpy()\n",
    "        \n",
    "        # Get transfer learning model prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = transfer_model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            logits = F.interpolate(logits, size=gt_labels.shape, mode=\"bilinear\", align_corners=False)\n",
    "            transfer_pred = torch.argmax(logits, dim=1)[0].cpu().numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        baseline_metrics = calculate_metrics(baseline_pred, gt_labels, val_dataset.num_classes)\n",
    "        transfer_metrics = calculate_metrics(transfer_pred, gt_labels, val_dataset.num_classes)\n",
    "        \n",
    "        all_baseline_metrics.append(baseline_metrics)\n",
    "        all_transfer_metrics.append(transfer_metrics)\n",
    "        \n",
    "        # Create colored masks\n",
    "        gt_colored = create_colored_mask(gt_labels, class_colors)\n",
    "        baseline_colored = create_colored_mask(baseline_pred, class_colors)\n",
    "        transfer_colored = create_colored_mask(transfer_pred, class_colors)\n",
    "        \n",
    "        # Create difference visualization\n",
    "        diff_mask = np.zeros_like(original_img)\n",
    "        # Areas where transfer is correct and baseline is wrong = bright green\n",
    "        diff_mask[(transfer_pred == gt_labels) & (baseline_pred != gt_labels)] = [0, 255, 0]\n",
    "        # Areas where baseline is correct and transfer is wrong = red\n",
    "        diff_mask[(baseline_pred == gt_labels) & (transfer_pred != gt_labels)] = [255, 0, 0]\n",
    "        \n",
    "        # Create publication-style visualization\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        gs = gridspec.GridSpec(2, 3, height_ratios=[1, 0.05])\n",
    "        \n",
    "        # Original image\n",
    "        ax1 = plt.subplot(gs[0, 0])\n",
    "        ax1.imshow(original_img)\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        ax2 = plt.subplot(gs[0, 1])\n",
    "        ax2.imshow(gt_colored)\n",
    "        ax2.set_title('Ground Truth')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # Baseline prediction\n",
    "        ax3 = plt.subplot(gs[0, 2])\n",
    "        im3 = ax3.imshow(baseline_colored)\n",
    "        ax3.set_title(f'Baseline (mIoU: {baseline_metrics[\"mean_iou\"]:.4f})')\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        # Transfer learning prediction with differences highlighted\n",
    "        ax4 = plt.subplot(gs[1, 0:2])\n",
    "        im4 = ax4.imshow(transfer_colored)\n",
    "        ax4.set_title(f'Transfer Learning (mIoU: {transfer_metrics[\"mean_iou\"]:.4f})')\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        # Improvement visualization\n",
    "        ax5 = plt.subplot(gs[1, 2])\n",
    "        im5 = ax5.imshow(diff_mask)\n",
    "        ax5.set_title('Differences\\nGreen: TL Better, Red: Baseline Better')\n",
    "        ax5.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(comparison_dir, f\"{dataset_name}_{i+1}_{image_name}.png\"), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create a closeup visualization of interesting regions\n",
    "        # This highlights specific improvements in challenging areas\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Find an interesting region with differences (if any)\n",
    "        diff_positions = np.where((diff_mask[:,:,0] > 0) | (diff_mask[:,:,1] > 0))\n",
    "        \n",
    "        if len(diff_positions[0]) > 0:\n",
    "            # Find center of differences\n",
    "            center_y = int(np.mean(diff_positions[0]))\n",
    "            center_x = int(np.mean(diff_positions[1]))\n",
    "            \n",
    "            # Define crop region\n",
    "            crop_size = min(200, gt_labels.shape[0]//2, gt_labels.shape[1]//2)\n",
    "            start_y = max(0, center_y - crop_size//2)\n",
    "            start_x = max(0, center_x - crop_size//2)\n",
    "            end_y = min(gt_labels.shape[0], start_y + crop_size)\n",
    "            end_x = min(gt_labels.shape[1], start_x + crop_size)\n",
    "            \n",
    "            # Crop images\n",
    "            crop_original = original_img[start_y:end_y, start_x:end_x]\n",
    "            crop_gt = gt_colored[start_y:end_y, start_x:end_x]\n",
    "            crop_baseline = baseline_colored[start_y:end_y, start_x:end_x]\n",
    "            crop_transfer = transfer_colored[start_y:end_y, start_x:end_x]\n",
    "            crop_diff = diff_mask[start_y:end_y, start_x:end_x]\n",
    "            \n",
    "            # Display cropped regions\n",
    "            axes[0, 0].imshow(original_img)\n",
    "            axes[0, 0].set_title('Original Image')\n",
    "            axes[0, 0].axis('off')\n",
    "            \n",
    "            # Show crop region\n",
    "            rect = plt.Rectangle((start_x, start_y), crop_size, crop_size, \n",
    "                                edgecolor='white', facecolor='none', linewidth=2)\n",
    "            axes[0, 0].add_patch(rect)\n",
    "            \n",
    "            axes[0, 1].imshow(crop_original)\n",
    "            axes[0, 1].set_title('Region of Interest')\n",
    "            axes[0, 1].axis('off')\n",
    "            \n",
    "            axes[0, 2].imshow(crop_gt)\n",
    "            axes[0, 2].set_title('Ground Truth')\n",
    "            axes[0, 2].axis('off')\n",
    "            \n",
    "            axes[1, 0].imshow(crop_baseline)\n",
    "            axes[1, 0].set_title('Baseline Prediction')\n",
    "            axes[1, 0].axis('off')\n",
    "            \n",
    "            axes[1, 1].imshow(crop_transfer)\n",
    "            axes[1, 1].set_title('Transfer Learning Prediction')\n",
    "            axes[1, 1].axis('off')\n",
    "            \n",
    "            axes[1, 2].imshow(crop_diff)\n",
    "            axes[1, 2].set_title('Differences')\n",
    "            axes[1, 2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(comparison_dir, f\"{dataset_name}_{i+1}_{image_name}_closeup.png\"), \n",
    "                        dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    # Create summary metrics visualization\n",
    "    baseline_mean_ious = [metrics['mean_iou'] for metrics in all_baseline_metrics]\n",
    "    transfer_mean_ious = [metrics['mean_iou'] for metrics in all_transfer_metrics]\n",
    "    \n",
    "    # Overall mIoU comparison\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(['Baseline', 'Transfer Learning'], \n",
    "            [np.mean(baseline_mean_ious), np.mean(transfer_mean_ious)],\n",
    "            color=['skyblue', 'coral'])\n",
    "    plt.ylabel('Mean IoU')\n",
    "    plt.title(f'{dataset_name} - Mean IoU Comparison')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.savefig(os.path.join(metrics_dir, f\"{dataset_name}_miou_comparison.png\"), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Per-class IoU comparison\n",
    "    class_ious_baseline = np.nanmean([m['class_ious'] for m in all_baseline_metrics], axis=0)\n",
    "    class_ious_transfer = np.nanmean([m['class_ious'] for m in all_transfer_metrics], axis=0)\n",
    "    \n",
    "    # Only include classes that appear in the samples\n",
    "    valid_classes = ~np.isnan(class_ious_baseline) & ~np.isnan(class_ious_transfer)\n",
    "    class_indices = np.where(valid_classes)[0]\n",
    "    \n",
    "    if len(class_indices) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = np.arange(len(class_indices))\n",
    "        width = 0.35\n",
    "        \n",
    "        baseline_bars = ax.bar(x - width/2, class_ious_baseline[class_indices], width, \n",
    "                              label='Baseline', color='skyblue')\n",
    "        transfer_bars = ax.bar(x + width/2, class_ious_transfer[class_indices], width,\n",
    "                               label='Transfer Learning', color='coral')\n",
    "        \n",
    "        ax.set_ylabel('IoU')\n",
    "        ax.set_title(f'{dataset_name} - Per-Class IoU Comparison')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([class_names[i] for i in class_indices], rotation=90)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(metrics_dir, f\"{dataset_name}_class_iou_comparison.png\"), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Calculate and save improvements\n",
    "        improvements = class_ious_transfer - class_ious_baseline\n",
    "        relative_improvements = np.zeros_like(improvements)\n",
    "        for i, (baseline, transfer) in enumerate(zip(class_ious_baseline, class_ious_transfer)):\n",
    "            if not np.isnan(baseline) and baseline > 0:\n",
    "                relative_improvements[i] = (transfer - baseline) / baseline * 100\n",
    "        \n",
    "        # Sort classes by improvement\n",
    "        improvement_data = []\n",
    "        for i in class_indices:\n",
    "            improvement_data.append({\n",
    "                'Class': class_names[i],\n",
    "                'Baseline_IoU': class_ious_baseline[i],\n",
    "                'Transfer_IoU': class_ious_transfer[i],\n",
    "                'Absolute_Improvement': improvements[i],\n",
    "                'Relative_Improvement (%)': relative_improvements[i]\n",
    "            })\n",
    "        \n",
    "        # Sort by relative improvement\n",
    "        improvement_data.sort(key=lambda x: x['Relative_Improvement (%)'], reverse=True)\n",
    "        \n",
    "        # Create a table for the paper\n",
    "        fig, ax = plt.subplots(figsize=(10, len(improvement_data)*0.4 + 1))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        table_data = [[d['Class'], \n",
    "                       f\"{d['Baseline_IoU']:.4f}\", \n",
    "                       f\"{d['Transfer_IoU']:.4f}\", \n",
    "                       f\"{d['Absolute_Improvement']:.4f}\", \n",
    "                       f\"{d['Relative_Improvement (%)']:.2f}%\"] \n",
    "                      for d in improvement_data]\n",
    "        \n",
    "        table = ax.table(cellText=table_data, \n",
    "                         colLabels=['Class', 'Baseline IoU', 'Transfer IoU', \n",
    "                                    'Abs. Improvement', 'Rel. Improvement'],\n",
    "                         loc='center', cellLoc='center')\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 1.5)\n",
    "        \n",
    "        # Highlight cells with significant improvements\n",
    "        for i, d in enumerate(improvement_data):\n",
    "            if d['Relative_Improvement (%)'] > 5:  # More than 5% improvement\n",
    "                for j in range(1, 5):\n",
    "                    table[(i+1, j)].set_facecolor('#d5f5e3')  # Light green\n",
    "                    \n",
    "        plt.title(f'{dataset_name} - Class-wise Improvements', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(metrics_dir, f\"{dataset_name}_improvement_table.png\"), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save as CSV for reference\n",
    "        df = pd.DataFrame(improvement_data)\n",
    "        df.to_csv(os.path.join(metrics_dir, f\"{dataset_name}_improvements.csv\"), index=False)\n",
    "    \n",
    "    # Save the legend for class colors\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create color swatches for legend\n",
    "    handles = []\n",
    "    for i in class_indices:\n",
    "        color = [c/255 for c in class_colors[i]]\n",
    "        handles.append(plt.Rectangle((0,0), 1, 1, color=color, label=class_names[i]))\n",
    "    \n",
    "    ax.legend(handles=handles, loc='center', ncol=2)\n",
    "    plt.title(f'{dataset_name} - Class Color Legend')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{dataset_name}_color_legend.png\"), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to generate visualizations for both datasets.\"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Generate visualizations for KITTI\n",
    "    print(\"\\nGenerating KITTI visualizations...\")\n",
    "    # Create KITTI dataset and models\n",
    "    # [Code to implement]\n",
    "    \n",
    "    # Generate visualizations for IDD\n",
    "    print(\"\\nGenerating IDD visualizations...\")\n",
    "    # Create IDD dataset and models\n",
    "    # [Code to implement]\n",
    "    \n",
    "    print(\"\\nVisualization generation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def1b140-a2cf-49b7-bc52-9397365ab0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/typing_extensions-4.8.0+computecanada-py3-none-any.whl\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 30] Read-only file system: '/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/__pycache__/typing_extensions.cpython-311.pyc'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: torch in /home/thatkar/.local/lib/python3.11/site-packages (2.6.0+computecanada)\n",
      "Requirement already satisfied: torchvision in /home/thatkar/.local/lib/python3.11/site-packages (0.21.0+computecanada)\n",
      "Requirement already satisfied: filelock in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages (from torch) (3.12.2)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.10.0 from https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /home/thatkar/.local/lib/python3.11/site-packages (from torch) (3.4.2+computecanada)\n",
      "Requirement already satisfied: jinja2 in /home/thatkar/.local/lib/python3.11/site-packages (from torch) (3.1.5+computecanada)\n",
      "Requirement already satisfied: fsspec in /home/thatkar/.local/lib/python3.11/site-packages (from torch) (2025.2.0+computecanada)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/thatkar/.local/lib/python3.11/site-packages (from torch) (1.13.1+computecanada)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/mpmath-1.3.0+computecanada-py3-none-any.whl (from sympy==1.13.1->torch)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/numpy-2.2.2+computecanada-cp311-cp311-linux_x86_64.whl (from torchvision)\n",
      "Requirement already satisfied: pillow-simd!=8.3.*,>=5.3.0 in /home/thatkar/.local/lib/python3.11/site-packages (from torchvision) (9.5.0.post2+computecanada)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thatkar/.local/lib/python3.11/site-packages (from jinja2->torch) (2.1.5+computecanada)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, numpy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 30] Read-only file system: '/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/site-packages/__pycache__/typing_extensions.cpython-311.pyc'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U typing_extensions==4.8.0\n",
    "!pip install -U torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5623a516-69d8-4e84-b1bb-e7c2111fe019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c42a48475cc42499ad9d68f1b774e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899eb563f64f48c8a98c9ee5bf701f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/339M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac4f5077e1449a195948ae7fe7e16c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/339M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([32, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/segformer_b5_camvid.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 248\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Initialize with your model path\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/segformer_b5_camvid.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 248\u001b[0m     viz \u001b[38;5;241m=\u001b[39m \u001b[43mSegFormerAttentionViz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Process an image\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/your/image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mSegFormerAttentionViz.__init__\u001b[0;34m(self, model_path, num_classes)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m SegformerForSemanticSegmentation\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia/segformer-b5-finetuned-cityscapes-1024-1024\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[1;32m     24\u001b[0m     ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load saved weights\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/segformer_b5_camvid.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SegFormerAttentionViz:\n",
    "    def __init__(self, model_path, num_classes=32):\n",
    "        \"\"\"\n",
    "        Initialize with a pretrained SegFormer model\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to saved model checkpoint\n",
    "            num_classes: Number of segmentation classes\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Load model\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\",\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        # Load saved weights\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Register hooks to capture attention\n",
    "        self.attention_maps = {}\n",
    "        self._register_hooks()\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward hooks to extract attention maps from all encoder blocks\"\"\"\n",
    "        for block_idx, block in enumerate(self.model.segformer.encoder.block):\n",
    "            for layer_idx, layer in enumerate(block):\n",
    "                if hasattr(layer, 'layer') and hasattr(layer.layer, 'attention'):\n",
    "                    attn_layer = layer.layer.attention\n",
    "                    \n",
    "                    def hook_fn(module, input, output, block_idx=block_idx, layer_idx=layer_idx):\n",
    "                        # Extract attention weights\n",
    "                        # Shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "                        self.attention_maps[f\"block{block_idx}_layer{layer_idx}\"] = output[1].detach()\n",
    "                    \n",
    "                    attn_layer.register_forward_hook(hook_fn)\n",
    "    \n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"Process an image and get segmentation and attention maps\"\"\"\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Get segmentation output and attention maps\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_tensor)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "        # Get predicted segmentation\n",
    "        seg_pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Keep original image for visualization\n",
    "        self.original_image = np.array(image.resize((512, 512)))\n",
    "        \n",
    "        return seg_pred, self.attention_maps\n",
    "    \n",
    "    def visualize_attention(self, block_idx, layer_idx, head_idx=0):\n",
    "        \"\"\"\n",
    "        Visualize attention map for a specific block, layer and attention head\n",
    "        \n",
    "        Args:\n",
    "            block_idx: Encoder block index\n",
    "            layer_idx: Layer index within the block\n",
    "            head_idx: Attention head index\n",
    "        \"\"\"\n",
    "        key = f\"block{block_idx}_layer{layer_idx}\"\n",
    "        if key not in self.attention_maps:\n",
    "            print(f\"No attention map found for {key}\")\n",
    "            return\n",
    "        \n",
    "        # Get attention map for specified head\n",
    "        attn_map = self.attention_maps[key][0, head_idx].cpu().numpy()\n",
    "        \n",
    "        # Reshape attention map to 2D grid based on feature map size\n",
    "        # This depends on the specific architecture and might need adjustment\n",
    "        h = w = int(np.sqrt(attn_map.shape[0]))\n",
    "        attn_map = attn_map.reshape(h, w, h, w)\n",
    "        \n",
    "        # Average over source tokens to get a 2D attention map\n",
    "        attn_map = attn_map.mean(axis=(0, 1))\n",
    "        \n",
    "        # Resize to match image dimensions\n",
    "        attn_map = cv2.resize(attn_map, (512, 512))\n",
    "        \n",
    "        # Normalize for visualization\n",
    "        attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(self.original_image)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(self.original_image)\n",
    "        plt.imshow(attn_map, alpha=0.5, cmap='jet')\n",
    "        plt.title(f\"Attention Map (Block {block_idx}, Layer {layer_idx}, Head {head_idx})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"attention_b{block_idx}_l{layer_idx}_h{head_idx}.png\", dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        return attn_map\n",
    "    \n",
    "    def visualize_multi_scale_attention(self):\n",
    "        \"\"\"Visualize attention maps from different layers to show multi-scale processing\"\"\"\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(3, 3, 1)\n",
    "        plt.imshow(self.original_image)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot attention from different blocks and layers\n",
    "        plot_idx = 2\n",
    "        for block_idx in range(4):  # Adjust based on your model\n",
    "            for layer_idx in [0]:  # Just the first layer of each block\n",
    "                key = f\"block{block_idx}_layer{layer_idx}\"\n",
    "                if key in self.attention_maps:\n",
    "                    # Get attention map for first head\n",
    "                    attn_map = self.attention_maps[key][0, 0].cpu().numpy()\n",
    "                    \n",
    "                    # Reshape and process\n",
    "                    h = w = int(np.sqrt(attn_map.shape[0]))\n",
    "                    attn_map = attn_map.reshape(h, w, h, w)\n",
    "                    attn_map = attn_map.mean(axis=(0, 1))\n",
    "                    attn_map = cv2.resize(attn_map, (512, 512))\n",
    "                    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "                    \n",
    "                    plt.subplot(3, 3, plot_idx)\n",
    "                    plt.imshow(self.original_image)\n",
    "                    plt.imshow(attn_map, alpha=0.5, cmap='jet')\n",
    "                    plt.title(f\"Block {block_idx}, Layer {layer_idx}\")\n",
    "                    plt.axis('off')\n",
    "                    plot_idx += 1\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"multi_scale_attention.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def compare_models(self, image_path, model_paths, model_names):\n",
    "        \"\"\"\n",
    "        Compare attention maps between different model variants (B3, B4, B5)\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input image\n",
    "            model_paths: List of paths to different model checkpoints\n",
    "            model_names: List of names for the models (e.g., \"B3\", \"B4\", \"B5\")\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(20, 5*len(model_paths)))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(len(model_paths)+1, 3, 1)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_resized = image.resize((512, 512))\n",
    "        plt.imshow(np.array(image_resized))\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # For each model\n",
    "        for i, (model_path, model_name) in enumerate(zip(model_paths, model_names)):\n",
    "            # Create a new instance with this model\n",
    "            viz = SegFormerAttentionViz(model_path, num_classes=32)\n",
    "            \n",
    "            # Process image\n",
    "            seg_pred, attention_maps = viz.process_image(image_path)\n",
    "            \n",
    "            # Plot segmentation\n",
    "            plt.subplot(len(model_paths)+1, 3, 3*i+4)\n",
    "            plt.imshow(self._colorize_segmentation(seg_pred))\n",
    "            plt.title(f\"{model_name} Segmentation\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Plot early layer attention\n",
    "            key = \"block0_layer0\"\n",
    "            if key in attention_maps:\n",
    "                attn_map = attention_maps[key][0, 0].cpu().numpy()\n",
    "                h = w = int(np.sqrt(attn_map.shape[0]))\n",
    "                attn_map = attn_map.reshape(h, w, h, w)\n",
    "                attn_map = attn_map.mean(axis=(0, 1))\n",
    "                attn_map = cv2.resize(attn_map, (512, 512))\n",
    "                attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "                \n",
    "                plt.subplot(len(model_paths)+1, 3, 3*i+5)\n",
    "                plt.imshow(np.array(image_resized))\n",
    "                plt.imshow(attn_map, alpha=0.5, cmap='jet')\n",
    "                plt.title(f\"{model_name} Early Layer Attention\")\n",
    "                plt.axis('off')\n",
    "            \n",
    "            # Plot late layer attention\n",
    "            key = \"block3_layer0\"\n",
    "            if key in attention_maps:\n",
    "                attn_map = attention_maps[key][0, 0].cpu().numpy()\n",
    "                h = w = int(np.sqrt(attn_map.shape[0]))\n",
    "                attn_map = attn_map.reshape(h, w, h, w)\n",
    "                attn_map = attn_map.mean(axis=(0, 1))\n",
    "                attn_map = cv2.resize(attn_map, (512, 512))\n",
    "                attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "                \n",
    "                plt.subplot(len(model_paths)+1, 3, 3*i+6)\n",
    "                plt.imshow(np.array(image_resized))\n",
    "                plt.imshow(attn_map, alpha=0.5, cmap='jet')\n",
    "                plt.title(f\"{model_name} Late Layer Attention\")\n",
    "                plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"model_comparison.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _colorize_segmentation(self, segmentation):\n",
    "        \"\"\"Convert segmentation indices to colors for visualization\"\"\"\n",
    "        # Create a colormap (this is a simple example, you might want a more sophisticated one)\n",
    "        colormap = np.random.randint(0, 256, (256, 3), dtype=np.uint8)\n",
    "        colormap[0] = [0, 0, 0]  # background\n",
    "        \n",
    "        # Apply colormap\n",
    "        colored_segmentation = colormap[segmentation]\n",
    "        \n",
    "        return colored_segmentation\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with your model path\n",
    "    model_path = \"path/to/segformer_b5_camvid.pth\"\n",
    "    viz = SegFormerAttentionViz(model_path, num_classes=32)\n",
    "    \n",
    "    # Process an image\n",
    "    image_path = \"path/to/your/image.jpg\"\n",
    "    seg_pred, attention_maps = viz.process_image(image_path)\n",
    "    \n",
    "    # Visualize attention for specific block/layer/head\n",
    "    viz.visualize_attention(block_idx=3, layer_idx=5, head_idx=0)\n",
    "    \n",
    "    # Visualize multi-scale attention maps\n",
    "    viz.visualize_multi_scale_attention()\n",
    "    \n",
    "    # Compare different model variants\n",
    "    model_paths = [\n",
    "        \"path/to/segformer_b3_camvid.pth\",\n",
    "        \"path/to/segformer_b4_camvid.pth\",\n",
    "        \"path/to/segformer_b5_camvid.pth\"\n",
    "    ]\n",
    "    model_names = [\"SegFormer-B3\", \"SegFormer-B4\", \"SegFormer-B5\"]\n",
    "    viz.compare_models(image_path, model_paths, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594485ec-eca0-4941-a8c3-71c8b0ada193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
