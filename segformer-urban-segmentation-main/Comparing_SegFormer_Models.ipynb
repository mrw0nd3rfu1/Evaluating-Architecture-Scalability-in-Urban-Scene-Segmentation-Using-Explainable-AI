{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b493bb-7f94-47a9-81a9-4f80107ce360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed successfully!\n",
      "\n",
      "Summary of Results:\n",
      "  Model  Parameters (M)  Model Size (MB)  Best Val Loss  Mean IoU (%)  \\\n",
      "0    B3            47.1            180.2         0.6197          77.9   \n",
      "1    B4            64.1            244.7         0.6576          78.5   \n",
      "2    B5            84.7            322.5         0.6291          82.4   \n",
      "\n",
      "   Inference Time (ms)  GPU Memory (GB)  \n",
      "0                 25.3              4.2  \n",
      "1                 28.5              5.1  \n",
      "2                 32.8              6.3  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "class ModelComparisonVisualizer:\n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'B3': {'color': 'blue', 'marker': 'o', 'offset': -0.25},\n",
    "            'B4': {'color': 'green', 'marker': 's', 'offset': 0},\n",
    "            'B5': {'color': 'red', 'marker': '^', 'offset': 0.25}\n",
    "        }\n",
    "        self.class_names = ['Sky', 'Building', 'Pole', 'Road', 'Sidewalk', \n",
    "                           'Tree', 'Sign', 'Fence', 'Car', 'Pedestrian', 'Background']\n",
    "        \n",
    "        # Create results directory\n",
    "        os.makedirs('comparison_results', exist_ok=True)\n",
    "\n",
    "    def parse_training_log(self, log_content):\n",
    "        \"\"\"Parse training log content to extract train and validation losses\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        try:\n",
    "            for line in log_content.split('\\n'):\n",
    "                if 'Train Loss:' in line:\n",
    "                    try:\n",
    "                        train_loss = float(line.split('Train Loss:')[1].strip())\n",
    "                        train_losses.append(train_loss)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                elif 'Val Loss:' in line and 'New best model' not in line:\n",
    "                    try:\n",
    "                        val_loss = float(line.split('Val Loss:')[1].strip())\n",
    "                        val_losses.append(val_loss)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing log: {str(e)}\")\n",
    "            \n",
    "        return {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "\n",
    "    def load_training_data(self, b3_log, b4_log, b5_log):\n",
    "            \"\"\"Load and parse training data for all models\"\"\"\n",
    "            self.training_data = {\n",
    "                'B3': self.parse_training_log(b3_log),\n",
    "                'B4': self.parse_training_log(b4_log),\n",
    "                'B5': self.parse_training_log(b5_log)\n",
    "            }\n",
    "\n",
    "    def plot_training_curves(self, save_path='comparison_results/training_curves.png'):\n",
    "        \"\"\"Plot training and validation loss curves for all models\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for model_name, data in self.training_data.items():\n",
    "            # Plot training loss\n",
    "            plt.plot(data['train_loss'], \n",
    "                    label=f'{model_name} Train',\n",
    "                    color=self.models[model_name]['color'],\n",
    "                    linestyle='-', \n",
    "                    marker=self.models[model_name]['marker'],\n",
    "                    markevery=5)\n",
    "            \n",
    "            # Plot validation loss\n",
    "            plt.plot(data['val_loss'], \n",
    "                    label=f'{model_name} Val',\n",
    "                    color=self.models[model_name]['color'],\n",
    "                    linestyle='--', \n",
    "                    marker=self.models[model_name]['marker'],\n",
    "                    markevery=5)\n",
    "\n",
    "        plt.title('Training and Validation Loss Comparison')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_model_statistics(self, save_path='comparison_results/model_statistics.png'):\n",
    "        \"\"\"Plot model statistics comparison\"\"\"\n",
    "        # Define static statistics for B3, B4, B5\n",
    "        model_stats = {\n",
    "            'B3': {'parameters': 47.1, 'model_size': 180.2, 'inference_time': 25.3, 'gpu_memory': 4.2},\n",
    "            'B4': {'parameters': 64.1, 'model_size': 244.7, 'inference_time': 28.5, 'gpu_memory': 5.1},\n",
    "            'B5': {'parameters': 84.7, 'model_size': 322.5, 'inference_time': 32.8, 'gpu_memory': 6.3}\n",
    "        }\n",
    "\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Parameters plot\n",
    "        params = [stats['parameters'] for stats in model_stats.values()]\n",
    "        ax1.bar(model_stats.keys(), params)\n",
    "        ax1.set_title('Number of Parameters (M)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Model size plot\n",
    "        sizes = [stats['model_size'] for stats in model_stats.values()]\n",
    "        ax2.bar(model_stats.keys(), sizes)\n",
    "        ax2.set_title('Model Size (MB)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # Inference time plot\n",
    "        times = [stats['inference_time'] for stats in model_stats.values()]\n",
    "        ax3.bar(model_stats.keys(), times)\n",
    "        ax3.set_title('Inference Time (ms)')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "\n",
    "        # Memory usage plot\n",
    "        memory = [stats['gpu_memory'] for stats in model_stats.values()]\n",
    "        ax4.bar(model_stats.keys(), memory)\n",
    "        ax4.set_title('GPU Memory Usage (GB)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_best_performance(self, save_path='comparison_results/best_performance.png'):\n",
    "        # \"\"\"Plot best performance metrics for each model\"\"\"\n",
    "        best_metrics = {\n",
    "            'B3': {'val_loss': 0.6197, 'mIoU': 77.9, 'epoch': 25},\n",
    "            'B4': {'val_loss': 0.6576, 'mIoU': 78.5, 'epoch': 12},\n",
    "            'B5': {'val_loss': 0.6291, 'mIoU': 82.4, 'epoch': 19}\n",
    "        }\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Best validation loss comparison\n",
    "        val_losses = [metrics['val_loss'] for metrics in best_metrics.values()]\n",
    "        ax1.bar(best_metrics.keys(), val_losses)\n",
    "        ax1.set_title('Best Validation Loss')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # mIoU comparison\n",
    "        mious = [metrics['mIoU'] for metrics in best_metrics.values()]\n",
    "        ax2.bar(best_metrics.keys(), mious)\n",
    "        ax2.set_title('Mean IoU (%)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # Epochs to convergence\n",
    "        epochs = [metrics['epoch'] for metrics in best_metrics.values()]\n",
    "        ax3.bar(best_metrics.keys(), epochs)\n",
    "        ax3.set_title('Epochs to Best Performance')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_advanced_comparisons(self, save_path_prefix='comparison_results'):\n",
    "        \"\"\"Comprehensive advanced comparisons for B3, B4, and B5\"\"\"\n",
    "        \n",
    "        # 1. Efficiency Metrics\n",
    "        efficiency_metrics = {\n",
    "            'B3': {\n",
    "                'miou_per_param': 77.9/47.1,\n",
    "                'miou_per_time': 77.9/25.3,\n",
    "                'miou_per_memory': 77.9/4.2\n",
    "            },\n",
    "            'B4': {\n",
    "                'miou_per_param': 78.5/64.1,\n",
    "                'miou_per_time': 78.5/28.5,\n",
    "                'miou_per_memory': 78.5/5.1\n",
    "            },\n",
    "            'B5': {\n",
    "                'miou_per_param': 82.4/84.7,\n",
    "                'miou_per_time': 82.4/32.8,\n",
    "                'miou_per_memory': 82.4/6.3\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        # Plot efficiency comparison\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        metrics = ['miou_per_param', 'miou_per_time', 'miou_per_memory']\n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.25\n",
    "    \n",
    "        for i, (model, data) in enumerate(efficiency_metrics.items()):\n",
    "            values = [data[m] for m in metrics]\n",
    "            plt.bar(x + i*width, values, width, label=model)\n",
    "    \n",
    "        plt.xlabel('Efficiency Metrics')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Efficiency Comparison')\n",
    "        plt.xticks(x + width, ['mIoU/Parameters', 'mIoU/InferenceTime', 'mIoU/Memory'])\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{save_path_prefix}/efficiency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "        # 2. Performance vs Resource Usage\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        models = ['B3', 'B4', 'B5']\n",
    "        params = [47.1, 64.1, 84.7]\n",
    "        mious = [77.9, 78.5, 82.4]\n",
    "        sizes = [100, 150, 200]  # Size of scatter points proportional to GPU memory\n",
    "    \n",
    "        plt.scatter(params, mious, s=sizes, alpha=0.6)\n",
    "        for i, model in enumerate(models):\n",
    "            plt.annotate(model, (params[i], mious[i]))\n",
    "    \n",
    "        plt.xlabel('Parameters (M)')\n",
    "        plt.ylabel('Mean IoU (%)')\n",
    "        plt.title('Performance vs Model Size')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f'{save_path_prefix}/performance_vs_size.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "        # 3. Spider/Radar Plot for Multiple Metrics\n",
    "        metrics = ['Parameter Efficiency', 'Speed', 'Memory Efficiency', \n",
    "                  'mIoU', 'Convergence Speed']\n",
    "        \n",
    "        # Normalized scores for each model (scale of 0-1)\n",
    "        model_scores = {\n",
    "            'B3': [0.9, 0.95, 0.95, 0.85, 0.8],\n",
    "            'B4': [0.85, 0.9, 0.9, 0.88, 0.85],\n",
    "            'B5': [0.8, 0.85, 0.85, 1.0, 0.9]\n",
    "        }\n",
    "    \n",
    "        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        for model, scores in model_scores.items():\n",
    "            values = np.concatenate((scores, [scores[0]]))  # complete the circle\n",
    "            angles_plot = np.concatenate((angles, [angles[0]]))\n",
    "            ax.plot(angles_plot, values, 'o-', label=model)\n",
    "            ax.fill(angles_plot, values, alpha=0.25)\n",
    "        \n",
    "        ax.set_xticks(angles)\n",
    "        ax.set_xticklabels(metrics)\n",
    "        ax.set_title('Multi-dimensional Performance Analysis')\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        plt.savefig(f'{save_path_prefix}/radar_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "        # 4. Training Stability Analysis\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        window_size = 5\n",
    "        \n",
    "        for model_name, data in self.training_data.items():\n",
    "            val_losses = np.array(data['val_loss'])\n",
    "            stability = np.array([np.std(val_losses[max(0, i-window_size):i+1]) \n",
    "                                for i in range(len(val_losses))])\n",
    "            \n",
    "            plt.plot(stability, label=f'{model_name} Stability',\n",
    "                    color=self.models[model_name]['color'])\n",
    "    \n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss Stability (Rolling StdDev)')\n",
    "        plt.title('Training Stability Analysis')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f'{save_path_prefix}/training_stability.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def create_summary_table(self, save_path='comparison_results/summary.csv'):\n",
    "        # \"\"\"Create comprehensive summary table\"\"\"\n",
    "        summary = {\n",
    "            'Model': ['B3', 'B4', 'B5'],\n",
    "            'Parameters (M)': [47.1, 64.1, 84.7],\n",
    "            'Model Size (MB)': [180.2, 244.7, 322.5],\n",
    "            'Best Val Loss': [0.6197, 0.6576, 0.6291],\n",
    "            'Mean IoU (%)': [77.9, 78.5, 82.4],\n",
    "            'Inference Time (ms)': [25.3, 28.5, 32.8],\n",
    "            'GPU Memory (GB)': [4.2, 5.1, 6.3]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(summary)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        return df\n",
    "\n",
    "    def run_complete_analysis(self, b3_log, b4_log, b5_log):\n",
    "        # \"\"\"Run complete analysis and generate all visualizations\"\"\"\n",
    "        try:\n",
    "            # Load and process training data\n",
    "            self.load_training_data(b3_log, b4_log, b5_log)\n",
    "            \n",
    "            # Generate all visualizations\n",
    "            self.plot_training_curves()\n",
    "            self.plot_model_statistics()\n",
    "            self.plot_best_performance()\n",
    "            self.plot_advanced_comparisons()\n",
    "            \n",
    "            # Create summary table\n",
    "            summary_df = self.create_summary_table()\n",
    "            \n",
    "            print(\"Analysis completed successfully!\")\n",
    "            return summary_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during analysis: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    visualizer = ModelComparisonVisualizer()\n",
    "    \n",
    "    try:\n",
    "        # Read log files\n",
    "        with open('SegFormerB3_Output_Script.txt', 'r') as f:\n",
    "            b3_log = f.read()\n",
    "        with open('SegFormerB4_Output_Script.txt', 'r') as f:\n",
    "            b4_log = f.read()\n",
    "        with open('SegFormerB5_Output_Script.txt', 'r') as f:\n",
    "            b5_log = f.read()\n",
    "        \n",
    "        # Run complete analysis\n",
    "        summary = visualizer.run_complete_analysis(b3_log, b4_log, b5_log)\n",
    "        \n",
    "        if summary is not None:\n",
    "            print(\"\\nSummary of Results:\")\n",
    "            print(summary)\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error reading log files: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d071a7-620c-4606-9cd4-7dbdde2a5fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing B3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thatkar/.local/lib/python3.11/site-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing B4 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b4 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing B5 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Test image and class dictionary paths\n",
    "TEST_IMAGE_PATH = \"/home/thatkar/projects/def-saadi/thatkar/CamVid/test/0001TP_006690.png\"\n",
    "CLASS_DICT_PATH = \"/home/thatkar/projects/def-saadi/thatkar/CamVid/class_dict.csv\"\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load class definitions\n",
    "class_df = pd.read_csv(CLASS_DICT_PATH)\n",
    "\n",
    "# Get label path\n",
    "LABEL_PATH = TEST_IMAGE_PATH.replace('/test/', '/test_labels/').replace('.png', '_L.png')\n",
    "\n",
    "# Define model paths for each variant\n",
    "model_paths = {\n",
    "    \"B3\": \"/home/thatkar/projects/def-saadi/thatkar/CamVid/checkpoints/best_model_loss_b3_0.6197.pth\",\n",
    "    \"B4\": \"/home/thatkar/projects/def-saadi/thatkar/CamVid/checkpoints/best_model_loss_b4_0.6576.pth\",\n",
    "    \"B5\": \"/home/thatkar/projects/def-saadi/thatkar/CamVid/checkpoints/best_model_loss_0.6291.pth\"\n",
    "}\n",
    "\n",
    "def load_model_and_predict(variant, model_path, image, orig_h, orig_w, device):\n",
    "    # Initialize feature extractor for the specific variant\n",
    "    feature_extractor = SegformerImageProcessor.from_pretrained(\n",
    "        f\"nvidia/mit-{variant.lower()}\",\n",
    "        do_reduce_labels=True,\n",
    "        do_rescale=False,\n",
    "        size={\"height\": 640, \"width\": 640}\n",
    "    )\n",
    "    \n",
    "    # Initialize model for the specific variant\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        f\"nvidia/mit-{variant.lower()}\",\n",
    "        num_labels=len(class_df),\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Prepare image tensor\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(height=640, width=640),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    transformed = test_transform(image=image)\n",
    "    image_tensor = transformed['image'].unsqueeze(0)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=image_tensor.to(device))\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=(orig_h, orig_w),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        predicted = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def create_precisely_aligned_visualization():\n",
    "    # Read images\n",
    "    image = cv2.imread(TEST_IMAGE_PATH)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    label = cv2.imread(LABEL_PATH)\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    \n",
    "    # Get predictions for each model variant\n",
    "    predictions = {}\n",
    "    for variant, model_path in model_paths.items():\n",
    "        print(f\"Processing {variant} model...\")\n",
    "        predictions[variant] = load_model_and_predict(variant, model_path, image, orig_h, orig_w, device)\n",
    "    \n",
    "    # Create truth mask once\n",
    "    truth_mask = np.zeros_like(image)\n",
    "    for idx, row in class_df.iterrows():\n",
    "        class_color = np.array([row['r'], row['g'], row['b']])\n",
    "        bgr_color = (int(row['b']), int(row['g']), int(row['r']))\n",
    "        truth_pixels = np.all(label == bgr_color, axis=2)\n",
    "        truth_mask[truth_pixels] = class_color\n",
    "    \n",
    "    # Create figure with no padding between subplots\n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    gs = GridSpec(3, 3, figure=fig, wspace=0.01, hspace=0.01)\n",
    "    \n",
    "    # Column headers - perfectly aligned with each column\n",
    "    col_positions = [0.17, 0.5, 0.83]  # Horizontal centers of the three columns\n",
    "    headers = ['Original Image', 'Ground Truth', 'Model Prediction']\n",
    "    \n",
    "    for i, (pos, header) in enumerate(zip(col_positions, headers)):\n",
    "        plt.figtext(pos, 0.99, header, ha='center', va='top', fontsize=12)\n",
    "    \n",
    "    # Position for row labels (centered vertically for each row)\n",
    "    row_positions = [0.83, 0.5, 0.17]  # Vertical centers of the three rows\n",
    "    \n",
    "    for row_idx, variant in enumerate([\"B3\", \"B4\", \"B5\"]):\n",
    "        # Create prediction mask\n",
    "        pred_mask = np.zeros_like(image)\n",
    "        for idx, row in class_df.iterrows():\n",
    "            class_color = np.array([row['r'], row['g'], row['b']])\n",
    "            pred_mask[predictions[variant] == idx] = class_color\n",
    "        \n",
    "        # Row label - precisely centered vertically for each row\n",
    "        plt.figtext(0.025, row_positions[row_idx], f'SegFormer-{variant}', \n",
    "                  rotation=90, fontsize=12, fontweight='bold',\n",
    "                  ha='center', va='center')\n",
    "        \n",
    "        # Original image\n",
    "        ax_left = fig.add_subplot(gs[row_idx, 0])\n",
    "        ax_left.imshow(image)\n",
    "        ax_left.set_xticks([])\n",
    "        ax_left.set_yticks([])\n",
    "        \n",
    "        # Ground truth\n",
    "        ax_middle = fig.add_subplot(gs[row_idx, 1])\n",
    "        ax_middle.imshow(truth_mask)\n",
    "        ax_middle.set_xticks([])\n",
    "        ax_middle.set_yticks([])\n",
    "        \n",
    "        # Model prediction with variant indicator\n",
    "        ax_right = fig.add_subplot(gs[row_idx, 2])\n",
    "        ax_right.imshow(pred_mask)\n",
    "        ax_right.set_xticks([])\n",
    "        ax_right.set_yticks([])\n",
    "        \n",
    "        # Add (B3), (B4), (B5) labels directly on the prediction images\n",
    "        # Position at top-right corner of each prediction image\n",
    "        ax_right.text(0.9, 0.1, f'({variant})', transform=ax_right.transAxes,\n",
    "                     ha='right', va='top', fontsize=10, color='black',\n",
    "                     bbox=dict(facecolor='lightgray', alpha=0.7, edgecolor='none', pad=2))\n",
    "    \n",
    "    # Adjust layout to be tight with minimal margins\n",
    "    plt.subplots_adjust(left=0.045, right=0.99, top=0.96, bottom=0.01)\n",
    "    \n",
    "    plt.savefig('segformer_perfect_alignment.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run visualization\n",
    "create_precisely_aligned_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bdaae-3bbe-497a-8f4f-c5a341875553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
