{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cccc511b-2ab6-4629-952f-3c5c3bdc2686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 1\n",
      "GPU Device Name: Tesla V100-SXM2-32GB\n",
      "Current GPU Device: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def check_gpu():\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"GPU Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Current GPU Device: {torch.cuda.current_device()}\")\n",
    "    else:\n",
    "        print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "check_gpu()\n",
    "\n",
    "# CUDA configs\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054f4690-9b84-4e19-962a-779808fc7527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 19:57:42.709983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-20 19:57:47.450200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-20 19:57:47.707792: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-20 19:57:47.827345: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-20 19:57:48.981971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-20 20:02:04.652034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/thatkar/jupyter1/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.4' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Basic imports first\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Wait a moment, then other imports\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Memory optimizations after PyTorch is fully imported\n",
    "cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Constants and Paths\n",
    "TRAIN_DIR = \"/home/thatkar/projects/def-saadi/thatkar/Cityscapes/leftImg8bit/train/\"\n",
    "TRAIN_LABELS_DIR = \"/home/thatkar/projects/def-saadi/thatkar/Cityscapes/gtFine/train/\"\n",
    "VAL_DIR = \"/home/thatkar/projects/def-saadi/thatkar/Cityscapes/leftImg8bit/val/\"\n",
    "VAL_LABELS_DIR = \"/home/thatkar/projects/def-saadi/thatkar/Cityscapes/gtFine/val/\"\n",
    "CHECKPOINT_DIR = \"/home/thatkar/projects/def-saadi/thatkar/Cityscapes/checkpoints/\"\n",
    "VISUALIZATION_DIR = \"/home/thatkar/projects/def-saadi/thatkar/Cityscapes/visualizations/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(VISUALIZATION_DIR, exist_ok=True)\n",
    "\n",
    "# Training Hyperparameters\n",
    "# IMAGE_HEIGHT = 1024\n",
    "# IMAGE_WIDTH = 2048\n",
    "IMAGE_HEIGHT = 512  # Half the original height\n",
    "IMAGE_WIDTH = 1024  # Half the original width\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "ACCUMULATION_STEPS = 16\n",
    "NUM_EPOCHS = 45\n",
    "LEARNING_RATE = 2e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP_VALUE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71dedbc8-1903-41dc-b974-7266670d76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes class definitions and color map\n",
    "CITYSCAPES_CLASSES = {\n",
    "    0: 'road',\n",
    "    1: 'sidewalk',\n",
    "    2: 'building',\n",
    "    3: 'wall',\n",
    "    4: 'fence',\n",
    "    5: 'pole',\n",
    "    6: 'traffic light',\n",
    "    7: 'traffic sign',\n",
    "    8: 'vegetation',\n",
    "    9: 'terrain',\n",
    "    10: 'sky',\n",
    "    11: 'person',\n",
    "    12: 'rider',\n",
    "    13: 'car',\n",
    "    14: 'truck',\n",
    "    15: 'bus',\n",
    "    16: 'train',\n",
    "    17: 'motorcycle',\n",
    "    18: 'bicycle'\n",
    "}\n",
    "\n",
    "# Data augmentation\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, feature_extractor, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.num_classes = len(CITYSCAPES_CLASSES)\n",
    "        \n",
    "        # Get all image files\n",
    "        for city_folder in os.listdir(image_dir):\n",
    "            city_path = os.path.join(image_dir, city_folder)\n",
    "            if os.path.isdir(city_path):\n",
    "                for img_file in glob.glob(os.path.join(city_path, \"*_leftImg8bit.png\")):\n",
    "                    # Construct corresponding label path\n",
    "                    base_name = os.path.basename(img_file).replace(\"_leftImg8bit.png\", \"\")\n",
    "                    label_name = f\"{base_name}_gtFine_labelIds.png\"\n",
    "                    label_city_path = os.path.join(label_dir, city_folder)\n",
    "                    label_path = os.path.join(label_city_path, label_name)\n",
    "                    \n",
    "                    if os.path.exists(label_path):\n",
    "                        self.images.append(img_file)\n",
    "                        self.labels.append(label_path)\n",
    "                    else:\n",
    "                        print(f\"Warning: Failed to load {img_file} or its label\")\n",
    "        \n",
    "        print(f\"Found {len(self.images)} valid image-label pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_labels(label_img):\n",
    "        \"\"\"\n",
    "        Convert Cityscapes labels to consecutive integers starting from 0\n",
    "        \"\"\"\n",
    "        # Cityscapes label mapping\n",
    "        label_mapping = {\n",
    "            7: 0,      # road\n",
    "            8: 1,      # sidewalk\n",
    "            11: 2,     # building\n",
    "            12: 3,     # wall\n",
    "            13: 4,     # fence\n",
    "            17: 5,     # pole\n",
    "            19: 6,     # traffic light\n",
    "            20: 7,     # traffic sign\n",
    "            21: 8,     # vegetation\n",
    "            22: 9,     # terrain\n",
    "            23: 10,    # sky\n",
    "            24: 11,    # person\n",
    "            25: 12,    # rider\n",
    "            26: 13,    # car\n",
    "            27: 14,    # truck\n",
    "            28: 15,    # bus\n",
    "            31: 16,    # train\n",
    "            32: 17,    # motorcycle\n",
    "            33: 18,    # bicycle\n",
    "        }\n",
    "        \n",
    "        label_copy = np.zeros_like(label_img)\n",
    "        for k, v in label_mapping.items():\n",
    "            label_copy[label_img == k] = v\n",
    "        \n",
    "        return label_copy\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load image\n",
    "            image = cv2.imread(self.images[idx])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Get label path using correct suffix\n",
    "            label_path = self.labels[idx]\n",
    "            label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if image is None or label is None:\n",
    "                raise ValueError(f\"Failed to load image or label at index {idx}\")\n",
    "            \n",
    "            # Convert labels to proper format\n",
    "            label = self.convert_labels(label)\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transforms:\n",
    "                transformed = self.transforms(image=image, mask=label)\n",
    "                image = transformed['image']\n",
    "                label = transformed['mask']\n",
    "            \n",
    "            return {\n",
    "                'pixel_values': image,\n",
    "                'labels': label.long()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data at index {idx}: {str(e)}\")\n",
    "            print(f\"Image path: {self.images[idx]}\")\n",
    "            print(f\"Label path: {label_path}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f7f5b0-176e-418f-b010-48fbdcd1e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSegmentationLoss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        return self.ce_loss(logits, labels)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Training Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in progress_bar:\n",
    "        try:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.amp.autocast('cuda', enabled=True):\n",
    "                outputs = model(pixel_values=pixel_values)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Resize logits to match label size if needed\n",
    "                if logits.shape[-2:] != labels.shape[-2:]:\n",
    "                    logits = F.interpolate(\n",
    "                        logits,\n",
    "                        size=labels.shape[-2:],\n",
    "                        mode=\"bilinear\",\n",
    "                        align_corners=False\n",
    "                    )\n",
    "                \n",
    "                loss = criterion(logits, labels) / ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VALUE)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                loss=loss.item() * ACCUMULATION_STEPS,\n",
    "                avg_loss=total_loss / (batch_idx + 1)\n",
    "            )\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "        try:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            if logits.shape[-2:] != labels.shape[-2:]:\n",
    "                logits = F.interpolate(\n",
    "                    logits,\n",
    "                    size=labels.shape[-2:],\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False\n",
    "                )\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during validation: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, save_path):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
    "    plt.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
    "    \n",
    "    window_size = 5\n",
    "    if len(train_losses) >= window_size:\n",
    "        train_ma = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        val_ma = np.convolve(val_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(range(window_size-1, len(train_losses)), train_ma, \n",
    "                '--', color='darkblue', alpha=0.5, label='Train Moving Avg')\n",
    "        plt.plot(range(window_size-1, len(val_losses)), val_ma, \n",
    "                '--', color='darkred', alpha=0.5, label='Val Moving Avg')\n",
    "    \n",
    "    plt.title('Training and Validation Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36dd42-61e9-4a6d-950b-a95fd350911a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thatkar/jupyter1/lib/python3.10/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 valid image-label pairs\n",
      "Found 500 valid image-label pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cacb83a5e240c9a49470cd52d430e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/2975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68a132254354aa9870dfd6149cc40da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1045\n",
      "Val Loss: 0.4626\n",
      "New best model saved! Val Loss: 0.4626\n",
      "\n",
      "Epoch 2/45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3cef6c107e42d285d9b0b1302b7823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/2975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4d24f514e741999b6aa9a96df51828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3523\n",
      "Val Loss: 0.3300\n",
      "New best model saved! Val Loss: 0.3300\n",
      "\n",
      "Epoch 3/45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c1fef207254e54b88446dd22f4c67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/2975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a471487141744c908272dfe6c9cea7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2485\n",
      "Val Loss: 0.2662\n",
      "New best model saved! Val Loss: 0.2662\n",
      "\n",
      "Epoch 4/45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe3b142653d45a68ec7594917c45fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/2975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9285b78cedc642f096757bbcdfa9a80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2119\n",
      "Val Loss: 0.2800\n",
      "\n",
      "Epoch 5/45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1594535b9ff54d128d3c4a993a3318a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/2975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd1badc90f0417089dd1487f0224499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # Clear CUDA cache before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize feature extractor\n",
    "        feature_extractor = SegformerImageProcessor.from_pretrained(\n",
    "            \"nvidia/mit-b5\",\n",
    "            do_reduce_labels=True,\n",
    "            do_rescale=False,\n",
    "            size={\"height\": IMAGE_HEIGHT, \"width\": IMAGE_WIDTH}\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = CityscapesDataset(TRAIN_DIR, TRAIN_LABELS_DIR, feature_extractor, transforms=train_transforms)\n",
    "        val_dataset = CityscapesDataset(VAL_DIR, VAL_LABELS_DIR, feature_extractor, transforms=val_transforms)\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/mit-b5\",\n",
    "            num_labels=train_dataset.num_classes,\n",
    "            id2label={str(i): CITYSCAPES_CLASSES[i] for i in range(len(CITYSCAPES_CLASSES))},\n",
    "            label2id={v: str(k) for k, v in CITYSCAPES_CLASSES.items()},\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        total_steps = len(train_loader) * NUM_EPOCHS // ACCUMULATION_STEPS\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=LEARNING_RATE,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "        \n",
    "        criterion = EnhancedSegmentationLoss(train_dataset.num_classes).to(device)\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, criterion, device, epoch)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, f'best_model_loss_{val_loss:.4f}.pth'))\n",
    "                print(f\"New best model saved! Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Plot training curves\n",
    "            plot_training_curves(\n",
    "                train_losses,\n",
    "                val_losses,\n",
    "                os.path.join(VISUALIZATION_DIR, f'training_curves_epoch_{epoch+1}.png')\n",
    "            )\n",
    "            \n",
    "            # Clear cache after each epoch\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2b81f-314e-4006-aa61-d7c95dfaa312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 06:04:05.319342: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-21 06:04:10.491943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-21 06:04:10.709502: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-21 06:04:10.878297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-21 06:04:11.959465: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Constants\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 1024\n",
    "\n",
    "# Cityscapes class definitions\n",
    "CITYSCAPES_CLASSES = {\n",
    "    0: 'road',\n",
    "    1: 'sidewalk',\n",
    "    2: 'building',\n",
    "    3: 'wall',\n",
    "    4: 'fence',\n",
    "    5: 'pole',\n",
    "    6: 'traffic light',\n",
    "    7: 'traffic sign',\n",
    "    8: 'vegetation',\n",
    "    9: 'terrain',\n",
    "    10: 'sky',\n",
    "    11: 'person',\n",
    "    12: 'rider',\n",
    "    13: 'car',\n",
    "    14: 'truck',\n",
    "    15: 'bus',\n",
    "    16: 'train',\n",
    "    17: 'motorcycle',\n",
    "    18: 'bicycle'\n",
    "}\n",
    "\n",
    "def convert_labels(label_img):\n",
    "    \"\"\"\n",
    "    Convert Cityscapes labels to consecutive integers starting from 0\n",
    "    \"\"\"\n",
    "    # Cityscapes label mapping\n",
    "    label_mapping = {\n",
    "        7: 0,      # road\n",
    "        8: 1,      # sidewalk\n",
    "        11: 2,     # building\n",
    "        12: 3,     # wall\n",
    "        13: 4,     # fence\n",
    "        17: 5,     # pole\n",
    "        19: 6,     # traffic light\n",
    "        20: 7,     # traffic sign\n",
    "        21: 8,     # vegetation\n",
    "        22: 9,     # terrain\n",
    "        23: 10,    # sky\n",
    "        24: 11,    # person\n",
    "        25: 12,    # rider\n",
    "        26: 13,    # car\n",
    "        27: 14,    # truck\n",
    "        28: 15,    # bus\n",
    "        31: 16,    # train\n",
    "        32: 17,    # motorcycle\n",
    "        33: 18,    # bicycle\n",
    "    }\n",
    "    \n",
    "    label_copy = np.zeros_like(label_img)\n",
    "    for k, v in label_mapping.items():\n",
    "        label_copy[label_img == k] = v\n",
    "    \n",
    "    return label_copy\n",
    "\n",
    "def visualize_prediction(model, image_path, label_path, feature_extractor, class_df, device):\n",
    "    # Read images\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    label = cv2.imread(label_path)\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    \n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    transformed = test_transform(image=image)\n",
    "    image_tensor = transformed['image'].unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=image_tensor.to(device))\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=(orig_h, orig_w),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        predicted = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Create visualization masks\n",
    "    pred_mask = np.zeros_like(image)\n",
    "    truth_mask = np.zeros_like(image)\n",
    "    \n",
    "    # Cityscapes color mapping\n",
    "    color_mapping = {\n",
    "        0: [128, 64, 128],   # road\n",
    "        1: [244, 35, 232],   # sidewalk\n",
    "        2: [70, 70, 70],     # building\n",
    "        3: [102, 102, 156],  # wall\n",
    "        4: [190, 153, 153],  # fence\n",
    "        5: [153, 153, 153],  # pole\n",
    "        6: [250, 170, 30],   # traffic light\n",
    "        7: [220, 220, 0],    # traffic sign\n",
    "        8: [107, 142, 35],   # vegetation\n",
    "        9: [152, 251, 152],  # terrain\n",
    "        10: [70, 130, 180],  # sky\n",
    "        11: [220, 20, 60],   # person\n",
    "        12: [255, 0, 0],     # rider\n",
    "        13: [0, 0, 142],     # car\n",
    "        14: [0, 0, 70],      # truck\n",
    "        15: [0, 60, 100],    # bus\n",
    "        16: [0, 80, 100],    # train\n",
    "        17: [0, 0, 230],     # motorcycle\n",
    "        18: [119, 11, 32]    # bicycle\n",
    "    }\n",
    "    \n",
    "    # Create prediction mask\n",
    "    for class_id, color in color_mapping.items():\n",
    "        pred_mask[predicted == class_id] = color\n",
    "        \n",
    "    # Create ground truth mask\n",
    "    label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "    label = convert_labels(label)\n",
    "    for class_id, color in color_mapping.items():\n",
    "        truth_mask[label == class_id] = color\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(truth_mask)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(pred_mask)\n",
    "    axes[2].set_title('Model Prediction')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "TEST_IMAGE_PATH = \"/home/thatkar/projects/def-saadi/thatkar/Cityscapes/leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png\"\n",
    "LABEL_PATH = TEST_IMAGE_PATH.replace('leftImg8bit', 'gtFine').replace('_leftImg8bit.png', '_gtFine_labelIds.png')\n",
    "best_model_path = \"/home/thatkar/projects/def-saadi/thatkar/Cityscapes/checkpoints/best_model_loss_0.2640.pth\"\n",
    "\n",
    "print(f\"Loading checkpoint from: {best_model_path}\")\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = SegformerImageProcessor.from_pretrained(\n",
    "    \"nvidia/mit-b5\",\n",
    "    do_reduce_labels=True,\n",
    "    do_rescale=False,\n",
    "    size={\"height\": IMAGE_HEIGHT, \"width\": IMAGE_WIDTH}\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b5\",\n",
    "    num_labels=len(CITYSCAPES_CLASSES),\n",
    "    id2label={str(i): str(i) for i in range(len(CITYSCAPES_CLASSES))},\n",
    "    label2id={str(i): i for i in range(len(CITYSCAPES_CLASSES))},\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Run visualization\n",
    "visualize_prediction(model, TEST_IMAGE_PATH, LABEL_PATH, feature_extractor, CITYSCAPES_CLASSES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae4f87-569e-486b-9e6a-6d835c937ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
