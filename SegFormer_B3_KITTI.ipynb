{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "93bc8ff0-0f46-44db-bf0d-2a1b266ed482",
      "metadata": {
        "id": "93bc8ff0-0f46-44db-bf0d-2a1b266ed482",
        "outputId": "1f62b719-63dd-413a-df26-d91b50320814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenCV version: 4.11.0\n",
            "PyTorch version: 2.6.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "print(f\"OpenCV version: {cv2.__version__}\")\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4264dabf-b553-4314-8c86-939cbf6b9423",
      "metadata": {
        "id": "4264dabf-b553-4314-8c86-939cbf6b9423",
        "outputId": "06945752-589d-4495-d8e1-1714cc11f78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: True\n",
            "Number of GPUs: 1\n",
            "GPU Device Name: Tesla V100-SXM2-32GB\n",
            "Current GPU Device: 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "212"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import gc\n",
        "\n",
        "def check_gpu():\n",
        "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "        print(f\"GPU Device Name: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Current GPU Device: {torch.cuda.current_device()}\")\n",
        "    else:\n",
        "        print(\"No GPU detected. Running on CPU.\")\n",
        "\n",
        "check_gpu()\n",
        "\n",
        "# CUDA configs\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a84ea5a-2706-43ac-8282-fc57a1724506",
      "metadata": {
        "id": "1a84ea5a-2706-43ac-8282-fc57a1724506",
        "tags": []
      },
      "source": [
        "# **Main Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7fea8e-636c-46c5-a302-db7659c8c186",
      "metadata": {
        "id": "1e7fea8e-636c-46c5-a302-db7659c8c186",
        "outputId": "acf88ec3-513b-4588-dc25-cf9e95050773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "About to start transfer learning...\n",
            "Generating comparison reports\n",
            "Performance comparison completed. Results saved to metrics directory.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.metrics import confusion_matrix, jaccard_score, accuracy_score\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Define paths - update these for your setup\n",
        "BASE_DIR = \"C:/Users/MSI GF66/Documents/Research ML/Tanmay\"  # Update this with your path\n",
        "ROOT_DIR = os.path.join(BASE_DIR, \"KITTI\")\n",
        "TRAIN_DIR = os.path.join(ROOT_DIR, \"training/image_2\")\n",
        "TRAIN_LABELS_DIR = os.path.join(ROOT_DIR, \"training/semantic\")\n",
        "VAL_DIR = os.path.join(ROOT_DIR, \"validation/image_2\")  # Will be created if it doesn't exist\n",
        "VAL_LABELS_DIR = os.path.join(ROOT_DIR, \"validation/semantic\")  # Will be created if it doesn't exist\n",
        "TEST_DIR = os.path.join(ROOT_DIR, \"testing/image_2\")\n",
        "CHECKPOINT_DIR = os.path.join(ROOT_DIR, \"checkpoints\")\n",
        "VISUALIZATION_DIR = os.path.join(ROOT_DIR, \"visualizations\")\n",
        "METRICS_DIR = os.path.join(ROOT_DIR, \"metrics\")\n",
        "# Path to your pre-trained CamVid model checkpoint\n",
        "CAMVID_CHECKPOINT = os.path.join(BASE_DIR, \"CamVid/checkpoints/best_model_loss_b3.pth\")  # Update with your model path\n",
        "\n",
        "# Training Hyperparameters\n",
        "IMAGE_HEIGHT = 1024  # KITTI images are typically larger\n",
        "IMAGE_WIDTH = 1024\n",
        "BATCH_SIZE = 1\n",
        "ACCUMULATION_STEPS = 16\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 2e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "GRAD_CLIP_VALUE = 1.0\n",
        "MODEL_TYPE = \"b3\"  # b3, b4, or b5 - should match your CamVid pre-trained model\n",
        "USE_REDUCED_CLASSES = True  # Use the reduced class set for better performance\n",
        "FREEZE_ENCODER = False  # Set to True to freeze encoder during transfer learning\n",
        "\n",
        "# KITTI Reduced Classes - easier to work with for the small dataset\n",
        "KITTI_REDUCED_CLASSES = {\n",
        "    0: {'name': 'background', 'color': (0, 0, 0)},  # Consolidate unlabeled classes\n",
        "    1: {'name': 'road', 'color': (128, 64, 128)},\n",
        "    2: {'name': 'sidewalk', 'color': (244, 35, 232)},\n",
        "    3: {'name': 'building', 'color': (70, 70, 70)},\n",
        "    4: {'name': 'wall', 'color': (102, 102, 156)},\n",
        "    5: {'name': 'fence', 'color': (190, 153, 153)},\n",
        "    6: {'name': 'pole', 'color': (153, 153, 153)},\n",
        "    7: {'name': 'traffic light', 'color': (250, 170, 30)},\n",
        "    8: {'name': 'traffic sign', 'color': (220, 220, 0)},\n",
        "    9: {'name': 'vegetation', 'color': (107, 142, 35)},\n",
        "    10: {'name': 'terrain', 'color': (152, 251, 152)},\n",
        "    11: {'name': 'sky', 'color': (70, 130, 180)},\n",
        "    12: {'name': 'person', 'color': (220, 20, 60)},\n",
        "    13: {'name': 'rider', 'color': (255, 0, 0)},\n",
        "    14: {'name': 'car', 'color': (0, 0, 142)},\n",
        "    15: {'name': 'truck', 'color': (0, 0, 70)},\n",
        "    16: {'name': 'bus', 'color': (0, 60, 100)},\n",
        "    17: {'name': 'motorcycle', 'color': (0, 0, 230)},\n",
        "    18: {'name': 'bicycle', 'color': (119, 11, 32)}\n",
        "}\n",
        "\n",
        "def create_validation_split(train_dir, label_dir, val_dir, val_label_dir, val_ratio=0.2, seed=42):\n",
        "    \"\"\"Create a validation split from the training data if it doesn't already exist.\"\"\"\n",
        "    # Create validation directories if they don't exist\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    os.makedirs(val_label_dir, exist_ok=True)\n",
        "\n",
        "    # Check if validation split already exists\n",
        "    if len(os.listdir(val_dir)) > 0:\n",
        "        print(f\"Validation split already exists with {len(os.listdir(val_dir))} images. Skipping split creation.\")\n",
        "        return\n",
        "\n",
        "    # Get all image files\n",
        "    image_files = [f for f in os.listdir(train_dir) if f.endswith('.png')]\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Determine number of validation samples\n",
        "    num_val = int(len(image_files) * val_ratio)\n",
        "\n",
        "    # Randomly select validation samples\n",
        "    val_samples = random.sample(image_files, num_val)\n",
        "\n",
        "    # Copy validation samples to validation directory\n",
        "    for img_file in val_samples:\n",
        "        # Copy image\n",
        "        shutil.copy(\n",
        "            os.path.join(train_dir, img_file),\n",
        "            os.path.join(val_dir, img_file)\n",
        "        )\n",
        "\n",
        "        # Try to find matching label file\n",
        "        label_found = False\n",
        "\n",
        "        # First try: direct matching filename\n",
        "        if os.path.exists(os.path.join(label_dir, img_file)):\n",
        "            shutil.copy(\n",
        "                os.path.join(label_dir, img_file),\n",
        "                os.path.join(val_label_dir, img_file)\n",
        "            )\n",
        "            label_found = True\n",
        "        else:\n",
        "            # Second try: check alternative label patterns\n",
        "            base_name = os.path.splitext(img_file)[0]\n",
        "            label_candidates = [\n",
        "                f\"{base_name}.png\",\n",
        "                f\"{base_name}_labelTrainIds.png\",\n",
        "                f\"{base_name}_labelIds.png\",\n",
        "                f\"{base_name}_gtFine_labelIds.png\"\n",
        "            ]\n",
        "\n",
        "            for label_name in label_candidates:\n",
        "                label_path = os.path.join(label_dir, label_name)\n",
        "                if os.path.exists(label_path):\n",
        "                    shutil.copy(\n",
        "                        label_path,\n",
        "                        os.path.join(val_label_dir, label_name)\n",
        "                    )\n",
        "                    label_found = True\n",
        "                    break\n",
        "\n",
        "        if not label_found:\n",
        "            print(f\"Warning: No label found for {img_file}\")\n",
        "\n",
        "    print(f\"Created validation split with {num_val} samples\")\n",
        "\n",
        "# Define data augmentation pipelines\n",
        "train_transforms = A.Compose([\n",
        "    A.RandomResizedCrop(\n",
        "        size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "        scale=(0.8, 1.0),\n",
        "        ratio=(0.75, 1.33),\n",
        "        p=1.0,\n",
        "        interpolation=cv2.INTER_LINEAR\n",
        "    ),\n",
        "    A.OneOf([\n",
        "        A.RandomBrightnessContrast(\n",
        "            brightness_limit=0.2,\n",
        "            contrast_limit=0.2,\n",
        "            p=0.5\n",
        "        ),\n",
        "        A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        A.RandomShadow(\n",
        "            shadow_roi=(0, 0.5, 1, 1),\n",
        "            p=0.5\n",
        "        ),\n",
        "    ], p=0.7),\n",
        "    A.OneOf([\n",
        "        A.Affine(\n",
        "            scale=(0.9, 1.1),\n",
        "            translate_percent={'x': (-0.1, 0.1), 'y': (-0.1, 0.1)},\n",
        "            rotate=(-15, 15),\n",
        "            border_mode=cv2.BORDER_CONSTANT\n",
        "        ),\n",
        "        A.ElasticTransform(\n",
        "            alpha=120,\n",
        "            sigma=6,\n",
        "            p=0.5\n",
        "        ),\n",
        "    ], p=0.5),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transforms = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, interpolation=cv2.INTER_LINEAR),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "class KITTIDataset(Dataset):\n",
        "    \"\"\"Dataset class for KITTI semantic segmentation.\"\"\"\n",
        "    def __init__(self, image_dir, label_dir, feature_extractor, transforms=None, use_reduced_classes=True):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transforms = transforms\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.ignore_index = 255\n",
        "        self.use_reduced_classes = use_reduced_classes\n",
        "\n",
        "        # Set up class mappings\n",
        "        self.class_mapping = KITTI_REDUCED_CLASSES\n",
        "        self.num_classes = len(KITTI_REDUCED_CLASSES)\n",
        "\n",
        "        # Create a mapping from original KITTI classes to reduced classes\n",
        "        self.original_to_reduced = {}\n",
        "        for i in range(35):  # KITTI has 35 classes\n",
        "            if i == 7:  # road\n",
        "                self.original_to_reduced[i] = 1\n",
        "            elif i == 8:  # sidewalk\n",
        "                self.original_to_reduced[i] = 2\n",
        "            elif i == 11:  # building\n",
        "                self.original_to_reduced[i] = 3\n",
        "            elif i == 12:  # wall\n",
        "                self.original_to_reduced[i] = 4\n",
        "            elif i == 13:  # fence\n",
        "                self.original_to_reduced[i] = 5\n",
        "            elif i == 17 or i == 18:  # pole/polegroup\n",
        "                self.original_to_reduced[i] = 6\n",
        "            elif i == 19:  # traffic light\n",
        "                self.original_to_reduced[i] = 7\n",
        "            elif i == 20:  # traffic sign\n",
        "                self.original_to_reduced[i] = 8\n",
        "            elif i == 21:  # vegetation\n",
        "                self.original_to_reduced[i] = 9\n",
        "            elif i == 22:  # terrain\n",
        "                self.original_to_reduced[i] = 10\n",
        "            elif i == 23:  # sky\n",
        "                self.original_to_reduced[i] = 11\n",
        "            elif i == 24:  # person\n",
        "                self.original_to_reduced[i] = 12\n",
        "            elif i == 25:  # rider\n",
        "                self.original_to_reduced[i] = 13\n",
        "            elif i == 26:  # car\n",
        "                self.original_to_reduced[i] = 14\n",
        "            elif i == 27:  # truck\n",
        "                self.original_to_reduced[i] = 15\n",
        "            elif i == 28:  # bus\n",
        "                self.original_to_reduced[i] = 16\n",
        "            elif i == 32:  # motorcycle\n",
        "                self.original_to_reduced[i] = 17\n",
        "            elif i == 33:  # bicycle\n",
        "                self.original_to_reduced[i] = 18\n",
        "            else:  # map to background\n",
        "                self.original_to_reduced[i] = 0\n",
        "\n",
        "        # Validate and filter images\n",
        "        self.images = []\n",
        "        valid_extensions = ('.png', '.jpg', '.jpeg')\n",
        "\n",
        "        print(f\"Validating images in {image_dir}...\")\n",
        "        all_images = [f for f in os.listdir(image_dir) if f.lower().endswith(valid_extensions)]\n",
        "\n",
        "        for img_name in all_images:\n",
        "            # First try: direct mapping (same filename for image and label)\n",
        "            img_path = os.path.join(image_dir, img_name)\n",
        "            label_path = os.path.join(label_dir, img_name)\n",
        "\n",
        "            if os.path.exists(label_path):\n",
        "                img_test = cv2.imread(img_path)\n",
        "                label_test = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\n",
        "                if img_test is not None and label_test is not None:\n",
        "                    self.images.append((img_name, img_name))\n",
        "                    continue\n",
        "\n",
        "            # Second try: check alternative label filenames\n",
        "            base_name = os.path.splitext(img_name)[0]\n",
        "            label_candidates = [\n",
        "                os.path.join(label_dir, f\"{base_name}.png\"),\n",
        "                os.path.join(label_dir, f\"{base_name}_labelTrainIds.png\"),\n",
        "                os.path.join(label_dir, f\"{base_name}_labelIds.png\"),\n",
        "                os.path.join(label_dir, f\"{base_name}_gtFine_labelIds.png\")\n",
        "            ]\n",
        "\n",
        "            valid_label_path = None\n",
        "            for label_path in label_candidates:\n",
        "                if os.path.exists(label_path):\n",
        "                    img_test = cv2.imread(img_path)\n",
        "                    label_test = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\n",
        "                    if img_test is not None and label_test is not None:\n",
        "                        valid_label_path = label_path\n",
        "                        break\n",
        "\n",
        "            if valid_label_path:\n",
        "                self.images.append((img_name, os.path.basename(valid_label_path)))\n",
        "            else:\n",
        "                print(f\"Warning: No valid label found for {img_name}\")\n",
        "\n",
        "        print(f\"Found {len(self.images)} valid image-label pairs in {image_dir}\")\n",
        "\n",
        "        if len(self.images) == 0:\n",
        "            raise RuntimeError(f\"No valid image-label pairs found in {image_dir}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Load and process input image\n",
        "            img_name, label_name = self.images[idx]\n",
        "            image_path = os.path.join(self.image_dir, img_name)\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Load and process label image\n",
        "            label_path = os.path.join(self.label_dir, label_name)\n",
        "\n",
        "            # Check if the label is color or single channel\n",
        "            label = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\n",
        "            if label is None:\n",
        "                raise ValueError(f\"Failed to load label: {label_path}\")\n",
        "\n",
        "            # Convert label to class indices\n",
        "            if len(label.shape) == 3:  # Color image (RGB)\n",
        "                h, w = label.shape[:2]\n",
        "                label_mask = np.zeros((h, w), dtype=np.int64)\n",
        "\n",
        "                for class_id, class_info in self.class_mapping.items():\n",
        "                    color = class_info['color']\n",
        "                    # RGB to BGR for comparison with OpenCV\n",
        "                    bgr_color = (color[2], color[1], color[0])\n",
        "                    mask = np.all(label == bgr_color, axis=2)\n",
        "                    label_mask[mask] = class_id\n",
        "            else:  # Already single channel with class IDs\n",
        "                label_mask = np.zeros_like(label, dtype=np.int64)\n",
        "\n",
        "                # Map original KITTI classes to our reduced class set\n",
        "                for original_id, reduced_id in self.original_to_reduced.items():\n",
        "                    label_mask[label == original_id] = reduced_id\n",
        "\n",
        "            # Apply transforms\n",
        "            if self.transforms:\n",
        "                try:\n",
        "                    transformed = self.transforms(image=image, mask=label_mask)\n",
        "                    image = transformed['image']\n",
        "                    label_mask = transformed['mask']\n",
        "                except Exception as e:\n",
        "                    print(f\"Transform error for image {img_name}: {str(e)}\")\n",
        "                    raise\n",
        "\n",
        "            return {\n",
        "                'pixel_values': image,\n",
        "                'labels': torch.as_tensor(label_mask, dtype=torch.long),\n",
        "                'image_name': img_name\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {self.images[idx][0]}: {str(e)}\")\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def get_class_names(self):\n",
        "        \"\"\"Return a list of class names in order.\"\"\"\n",
        "        return [self.class_mapping[i]['name'] for i in range(self.num_classes)]\n",
        "\n",
        "    def get_color_map(self):\n",
        "        \"\"\"Return a mapping of class IDs to colors for visualization.\"\"\"\n",
        "        return {i: self.class_mapping[i]['color'] for i in range(self.num_classes)}\n",
        "\n",
        "class EnhancedSegmentationLoss(nn.Module):\n",
        "    \"\"\"Enhanced loss function with boundary-aware components for semantic segmentation.\"\"\"\n",
        "    def __init__(self, num_classes, ignore_index=255, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "        # Use provided class weights or create default ones\n",
        "        if class_weights is None:\n",
        "            # Base weights - all classes equal\n",
        "            class_weights = torch.ones(num_classes)\n",
        "\n",
        "            # Enhance weights for rare or difficult classes\n",
        "            rare_classes = [7, 8, 12, 13, 17, 18]  # traffic light, sign, person, rider, motorcycle, bicycle\n",
        "            small_objects = [6, 7, 8, 12, 13, 17, 18]  # poles, traffic lights, signs, pedestrians, bikes\n",
        "            vehicle_classes = [14, 15, 16]  # car, truck, bus\n",
        "\n",
        "            for class_idx in range(num_classes):\n",
        "                if class_idx in small_objects:\n",
        "                    class_weights[class_idx] = 3.0\n",
        "                elif class_idx in vehicle_classes:\n",
        "                    class_weights[class_idx] = 2.0\n",
        "                elif class_idx in rare_classes:\n",
        "                    class_weights[class_idx] = 2.5\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights, ignore_index=ignore_index)\n",
        "        self.smooth = 1e-5\n",
        "\n",
        "    def get_boundaries(self, tensor):\n",
        "        \"\"\"Extract boundaries from semantic masks for boundary-aware loss.\"\"\"\n",
        "        boundaries = torch.zeros_like(tensor, dtype=torch.float)\n",
        "        kernel_sizes = [3, 5, 7, 9]\n",
        "        weights = [0.4, 0.3, 0.2, 0.1]\n",
        "\n",
        "        for k_size, weight in zip(kernel_sizes, weights):\n",
        "            pooled = F.max_pool2d(\n",
        "                tensor.float(),\n",
        "                kernel_size=k_size,\n",
        "                stride=1,\n",
        "                padding=k_size//2\n",
        "            )\n",
        "            boundaries += weight * (pooled != tensor.float()).float()\n",
        "        return boundaries\n",
        "\n",
        "    def calculate_iou_loss(self, pred, target):\n",
        "        \"\"\"Calculate IoU loss for better boundary prediction.\"\"\"\n",
        "        pred = F.softmax(pred, dim=1)\n",
        "        pred = pred.flatten(2)\n",
        "        target = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2).flatten(2)\n",
        "\n",
        "        intersection = (pred * target).sum(-1)\n",
        "        total = (pred + target).sum(-1)\n",
        "        union = total - intersection\n",
        "        valid_mask = union > self.smooth\n",
        "        iou = torch.zeros_like(intersection)\n",
        "        iou[valid_mask] = (intersection[valid_mask] + self.smooth) / (union[valid_mask] + self.smooth)\n",
        "\n",
        "        return 1 - iou.mean()\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\"Combined loss function with weighted components.\"\"\"\n",
        "        ce_loss = self.ce_loss(outputs, targets)\n",
        "        iou_loss = self.calculate_iou_loss(outputs, targets)\n",
        "\n",
        "        edges = self.get_boundaries(targets)\n",
        "        pred_edges = self.get_boundaries(torch.argmax(outputs, dim=1))\n",
        "        boundary_loss = F.mse_loss(pred_edges, edges)\n",
        "\n",
        "        total_loss = ce_loss + 0.4 * iou_loss + 0.8 * boundary_loss\n",
        "\n",
        "        return total_loss, {'ce_loss': ce_loss.item(),\n",
        "                           'iou_loss': iou_loss.item(),\n",
        "                           'boundary_loss': boundary_loss.item()}\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, scaler, criterion, device, epoch):\n",
        "    \"\"\"Train model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batch_losses = []\n",
        "    component_losses = {'ce_loss': 0, 'iou_loss': 0, 'boundary_loss': 0}\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        try:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with torch.amp.autocast(device_type=str(device), dtype=torch.float16):\n",
        "                outputs = model(pixel_values=pixel_values)\n",
        "                logits = outputs.logits\n",
        "\n",
        "                logits = F.interpolate(\n",
        "                    logits,\n",
        "                    size=labels.shape[-2:],\n",
        "                    mode=\"bilinear\",\n",
        "                    align_corners=False\n",
        "                )\n",
        "\n",
        "                loss, loss_components = criterion(logits, labels)\n",
        "                loss = loss / ACCUMULATION_STEPS\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VALUE)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "\n",
        "            # Track losses\n",
        "            current_loss = loss.item() * ACCUMULATION_STEPS\n",
        "            epoch_loss += current_loss\n",
        "            batch_losses.append(current_loss)\n",
        "\n",
        "            # Track component losses\n",
        "            for k, v in loss_components.items():\n",
        "                component_losses[k] += v\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': current_loss,\n",
        "                'avg_loss': epoch_loss / (batch_idx + 1)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Average component losses\n",
        "    for k in component_losses:\n",
        "        component_losses[k] /= len(train_loader)\n",
        "\n",
        "    return epoch_loss / len(train_loader), batch_losses, component_losses\n",
        "\n",
        "def calculate_metrics(predictions, ground_truths, num_classes, ignore_index=255):\n",
        "    \"\"\"Calculate comprehensive segmentation metrics including mIoU.\"\"\"\n",
        "    # Flatten tensors for metric calculation\n",
        "    preds = np.concatenate([p.numpy().flatten() for p in predictions])\n",
        "    gts = np.concatenate([g.numpy().flatten() for g in ground_truths])\n",
        "\n",
        "    # Filter out ignored pixels\n",
        "    valid_idx = gts != ignore_index\n",
        "    preds = preds[valid_idx]\n",
        "    gts = gts[valid_idx]\n",
        "\n",
        "    # Overall pixel accuracy\n",
        "    pixel_acc = accuracy_score(gts, preds)\n",
        "\n",
        "    # Class-wise metrics\n",
        "    class_ious = jaccard_score(gts, preds, average=None, labels=range(num_classes), zero_division=0)\n",
        "\n",
        "    # Mean IoU\n",
        "    mean_iou = np.mean(class_ious)\n",
        "\n",
        "    # Confusion matrix\n",
        "    conf_mat = confusion_matrix(gts, preds, labels=range(num_classes))\n",
        "\n",
        "    # Class-wise precision and recall\n",
        "    precision = np.zeros(num_classes)\n",
        "    recall = np.zeros(num_classes)\n",
        "    f1_score = np.zeros(num_classes)\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        true_pos = conf_mat[i, i]\n",
        "        false_pos = conf_mat[:, i].sum() - true_pos\n",
        "        false_neg = conf_mat[i, :].sum() - true_pos\n",
        "\n",
        "        # Calculate precision and recall with handling for division by zero\n",
        "        if true_pos + false_pos > 0:\n",
        "            precision[i] = true_pos / (true_pos + false_pos)\n",
        "        if true_pos + false_neg > 0:\n",
        "            recall[i] = true_pos / (true_pos + false_neg)\n",
        "        if precision[i] + recall[i] > 0:\n",
        "            f1_score[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
        "\n",
        "    # Frequency weighted IoU\n",
        "    class_freq = np.bincount(gts, minlength=num_classes) / len(gts)\n",
        "    freq_weighted_iou = (class_ious * class_freq).sum()\n",
        "\n",
        "    metrics = {\n",
        "        'pixel_accuracy': pixel_acc,\n",
        "        'mean_iou': mean_iou,\n",
        "        'class_iou': class_ious.tolist(),\n",
        "        'precision': precision.tolist(),\n",
        "        'recall': recall.tolist(),\n",
        "        'f1_score': f1_score.tolist(),\n",
        "        'freq_weighted_iou': freq_weighted_iou,\n",
        "        'confusion_matrix': conf_mat.tolist()\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, val_loader, criterion, device, dataset):\n",
        "    \"\"\"Validate model on validation set.\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    component_losses = {'ce_loss': 0, 'iou_loss': 0, 'boundary_loss': 0}\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "    image_names = []\n",
        "\n",
        "    for batch in tqdm(val_loader, desc='Validation'):\n",
        "        try:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            image_names.extend(batch['image_name'])\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            logits = F.interpolate(\n",
        "                logits,\n",
        "                size=labels.shape[-2:],\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False\n",
        "            )\n",
        "\n",
        "            loss, loss_comps = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Track component losses\n",
        "            for k, v in loss_comps.items():\n",
        "                component_losses[k] += v\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).cpu()\n",
        "            predictions.append(preds)\n",
        "            ground_truths.append(labels.cpu())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during validation: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Average component losses\n",
        "    for k in component_losses:\n",
        "        component_losses[k] /= len(val_loader)\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    metrics = calculate_metrics(predictions, ground_truths, dataset.num_classes)\n",
        "    metrics['val_loss'] = val_loss / len(val_loader)\n",
        "    metrics['component_losses'] = component_losses\n",
        "\n",
        "    return metrics, predictions, ground_truths, image_names\n",
        "\n",
        "def visualize_predictions(images, predictions, ground_truths, class_colors, class_names, output_dir, limit=10):\n",
        "    \"\"\"Visualize model predictions compared to ground truth.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create color maps for visualization\n",
        "    color_map = np.zeros((max(class_colors.keys()) + 1, 3), dtype=np.uint8)\n",
        "    for class_id, color in class_colors.items():\n",
        "        color_map[class_id] = color\n",
        "\n",
        "    for i, (image_name, pred, gt) in enumerate(zip(images, predictions, ground_truths)):\n",
        "        if i >= limit:\n",
        "            break\n",
        "\n",
        "        # Get shapes and ensure they're 2D\n",
        "        if len(pred.shape) == 3:\n",
        "            pred = pred[0]  # Take the first item if batched\n",
        "        if len(gt.shape) == 3:\n",
        "            gt = gt[0]  # Take the first item if batched\n",
        "\n",
        "        # Create colored predictions and ground truth\n",
        "        pred_colored = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
        "        gt_colored = np.zeros((gt.shape[0], gt.shape[1], 3), dtype=np.uint8)\n",
        "\n",
        "        # Apply color map\n",
        "        for class_id, color in class_colors.items():\n",
        "            pred_mask = (pred == class_id)\n",
        "            gt_mask = (gt == class_id)\n",
        "\n",
        "            for c in range(3):\n",
        "                pred_colored[:, :, c][pred_mask] = color[c]\n",
        "                gt_colored[:, :, c][gt_mask] = color[c]\n",
        "\n",
        "        # Create visualization grid\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "        axes[0].imshow(gt_colored)\n",
        "        axes[0].set_title(\"Ground Truth\")\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        axes[1].imshow(pred_colored)\n",
        "        axes[1].set_title(\"Prediction\")\n",
        "        axes[1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, f\"vis_{image_name}\"), dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "    # Create a legend for class colors\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    patches = []\n",
        "    for class_id, class_name in enumerate(class_names):\n",
        "        if class_id in class_colors:\n",
        "            color = np.array(class_colors[class_id]) / 255.0\n",
        "            patches.append(plt.Rectangle((0, 0), 1, 1, fc=color, label=class_name))\n",
        "\n",
        "    ax.legend(handles=patches, loc='center', ncol=2)\n",
        "    ax.set_axis_off()\n",
        "    plt.savefig(os.path.join(output_dir, \"class_legend.png\"), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def plot_training_curves(train_losses, val_losses, component_losses, metrics, save_dir):\n",
        "    \"\"\"Plot training curves and metrics.\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Plot overall losses\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
        "    plt.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
        "\n",
        "    window_size = 5\n",
        "    if len(train_losses) >= window_size:\n",
        "        train_ma = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
        "        val_ma = np.convolve(val_losses, np.ones(window_size)/window_size, mode='valid')\n",
        "        plt.plot(range(window_size-1, len(train_losses)), train_ma,\n",
        "                '--', color='darkblue', alpha=0.5, label='Train Moving Avg')\n",
        "        plt.plot(range(window_size-1, len(val_losses)), val_ma,\n",
        "                '--', color='darkred', alpha=0.5, label='Val Moving Avg')\n",
        "\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(os.path.join(save_dir, 'loss_curves.png'), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot component losses\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot([d['ce_loss'] for d in component_losses['train']], label='CE Loss (Train)', color='blue')\n",
        "    plt.plot([d['iou_loss'] for d in component_losses['train']], label='IoU Loss (Train)', color='green')\n",
        "    plt.plot([d['boundary_loss'] for d in component_losses['train']], label='Boundary Loss (Train)', color='orange')\n",
        "    plt.plot([d['ce_loss'] for d in component_losses['val']], label='CE Loss (Val)', color='blue', linestyle='--')\n",
        "    plt.plot([d['iou_loss'] for d in component_losses['val']], label='IoU Loss (Val)', color='green', linestyle='--')\n",
        "    plt.plot([d['boundary_loss'] for d in component_losses['val']], label='Boundary Loss (Val)', color='orange', linestyle='--')\n",
        "\n",
        "    plt.title('Loss Components Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(os.path.join(save_dir, 'component_losses.png'), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot metrics\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(metrics['mean_iou'], label='Mean IoU', color='blue', marker='o')\n",
        "    plt.plot(metrics['pixel_accuracy'], label='Pixel Accuracy', color='green', marker='s')\n",
        "    plt.plot(metrics['freq_weighted_iou'], label='Freq Weighted IoU', color='red', marker='^')\n",
        "\n",
        "    plt.title('Segmentation Metrics Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.savefig(os.path.join(save_dir, 'metrics.png'), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot per-class IoU\n",
        "    if len(metrics['class_iou']) > 0:\n",
        "        class_names = [f\"Class {i}\" for i in range(len(metrics['class_iou'][0]))]\n",
        "        last_class_iou = metrics['class_iou'][-1]\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.bar(class_names, last_class_iou)\n",
        "        plt.title('IoU by Class (Final Epoch)')\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('IoU')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(True, alpha=0.3, axis='y')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'class_iou.png'), dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "def evaluate_transfer_learning_performance():\n",
        "    \"\"\"Compare performance of baseline and transfer learning.\"\"\"\n",
        "    # Load metrics from the best checkpoints\n",
        "    transfer_metrics_path = os.path.join(METRICS_DIR, 'transfer/best_metrics.json')\n",
        "    baseline_metrics_path = os.path.join(METRICS_DIR, 'baseline/best_metrics.json')\n",
        "\n",
        "    if os.path.exists(transfer_metrics_path) and os.path.exists(baseline_metrics_path):\n",
        "        with open(transfer_metrics_path, 'r') as f:\n",
        "            transfer_metrics = json.load(f)\n",
        "\n",
        "        with open(baseline_metrics_path, 'r') as f:\n",
        "            baseline_metrics = json.load(f)\n",
        "\n",
        "        # Create comparison directory\n",
        "        comparison_dir = os.path.join(VISUALIZATION_DIR, 'comparison')\n",
        "        os.makedirs(comparison_dir, exist_ok=True)\n",
        "\n",
        "        # Plot overall metrics comparison\n",
        "        metrics_to_compare = ['mean_iou', 'pixel_accuracy', 'freq_weighted_iou']\n",
        "        values_transfer = [transfer_metrics[m] for m in metrics_to_compare]\n",
        "        values_baseline = [baseline_metrics[m] for m in metrics_to_compare]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        x = np.arange(len(metrics_to_compare))\n",
        "        width = 0.35\n",
        "\n",
        "        plt.bar(x - width/2, values_transfer, width, label='With Transfer Learning')\n",
        "        plt.bar(x + width/2, values_baseline, width, label='From Scratch')\n",
        "\n",
        "        plt.xticks(x, [m.replace('_', ' ').title() for m in metrics_to_compare])\n",
        "        plt.ylim(0, 1)\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Performance Comparison: Transfer Learning vs Training from Scratch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3, axis='y')\n",
        "        plt.savefig(os.path.join(comparison_dir, 'metrics_comparison.png'), dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "        # Create per-class IoU comparison\n",
        "        # class_names = [f\"Class {i}\" for i in range(len(transfer_metrics['class_iou']))]\n",
        "        class_names = [KITTI_REDUCED_CLASSES[i]['name'] for i in range(len(transfer_metrics['class_iou']))]\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        x = np.arange(len(class_names))\n",
        "        width = 0.35\n",
        "\n",
        "        plt.bar(x - width/2, transfer_metrics['class_iou'], width, label='With Transfer Learning')\n",
        "        plt.bar(x + width/2, baseline_metrics['class_iou'], width, label='From Scratch')\n",
        "\n",
        "        plt.xticks(x, class_names, rotation=45, ha='right')\n",
        "        plt.ylim(0, 1)\n",
        "        plt.ylabel('IoU Score')\n",
        "        plt.title('Per-Class IoU Comparison: Transfer Learning vs Training from Scratch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3, axis='y')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(comparison_dir, 'class_iou_comparison.png'), dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "        # Generate report\n",
        "        with open(os.path.join(METRICS_DIR, 'performance_comparison.txt'), 'w') as f:\n",
        "            f.write(\"# SegFormer Transfer Learning Performance Comparison\\n\\n\")\n",
        "\n",
        "            f.write(\"## Overall Metrics\\n\\n\")\n",
        "            f.write(\"| Metric | With Transfer Learning | From Scratch | Improvement |\\n\")\n",
        "            f.write(\"|--------|------------------------|--------------|-------------|\\n\")\n",
        "\n",
        "            for metric in metrics_to_compare:\n",
        "                tl_value = transfer_metrics[metric]\n",
        "                baseline_value = baseline_metrics[metric]\n",
        "                improvement = tl_value - baseline_value\n",
        "                improvement_percent = (improvement / baseline_value) * 100 if baseline_value > 0 else float('inf')\n",
        "\n",
        "                f.write(f\"| {metric.replace('_', ' ').title()} | {tl_value:.4f} | {baseline_value:.4f} | {improvement_percent:+.2f}% |\\n\")\n",
        "\n",
        "            f.write(\"\\n## Classes with Most Improvement\\n\\n\")\n",
        "            f.write(\"| Class | With Transfer Learning | From Scratch | Improvement |\\n\")\n",
        "            f.write(\"|-------|------------------------|--------------|-------------|\\n\")\n",
        "\n",
        "            # Calculate improvement for each class\n",
        "            improvements = []\n",
        "            for i, (tl, baseline) in enumerate(zip(transfer_metrics['class_iou'], baseline_metrics['class_iou'])):\n",
        "                if baseline > 0:\n",
        "                    improvement_percent = (tl - baseline) / baseline * 100\n",
        "                else:\n",
        "                    improvement_percent = float('inf') if tl > 0 else 0\n",
        "\n",
        "                improvements.append((i, tl, baseline, improvement_percent))\n",
        "\n",
        "            # Sort by improvement (descending)\n",
        "            improvements.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "            # Write top 5 most improved classes\n",
        "            for i, tl, baseline, imp_percent in improvements[:5]:\n",
        "                f.write(f\"| Class {i} | {tl:.4f} | {baseline:.4f} | {imp_percent:+.2f}% |\\n\")\n",
        "\n",
        "        print(\"Performance comparison completed. Results saved to metrics directory.\")\n",
        "    else:\n",
        "        print(\"Metrics files not found. Run both baseline and transfer learning experiments first.\")\n",
        "\n",
        "def train_and_evaluate(experiment_type):\n",
        "    \"\"\"\n",
        "    Train and evaluate SegFormer model on KITTI.\n",
        "    experiment_type: 'baseline' (from scratch) or 'transfer' (using CamVid weights)\n",
        "    \"\"\"\n",
        "    # Set random seeds for reproducibility\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "    # Clear CUDA cache before starting\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Set up experiment directories\n",
        "    exp_checkpoint_dir = os.path.join(CHECKPOINT_DIR, experiment_type)\n",
        "    exp_visualization_dir = os.path.join(VISUALIZATION_DIR, experiment_type)\n",
        "    exp_metrics_dir = os.path.join(METRICS_DIR, experiment_type)\n",
        "\n",
        "    # Create directories\n",
        "    for directory in [exp_checkpoint_dir, exp_visualization_dir, exp_metrics_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Get device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create validation split if it doesn't exist\n",
        "    if not os.path.exists(VAL_DIR) or len(os.listdir(VAL_DIR)) == 0:\n",
        "        print(\"Creating validation split...\")\n",
        "        create_validation_split(\n",
        "            TRAIN_DIR,\n",
        "            TRAIN_LABELS_DIR,\n",
        "            VAL_DIR,\n",
        "            VAL_LABELS_DIR,\n",
        "            val_ratio=0.2\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        # Initialize feature extractor\n",
        "        feature_extractor = SegformerImageProcessor.from_pretrained(\n",
        "            f\"nvidia/mit-{MODEL_TYPE}\",\n",
        "            do_reduce_labels=True,\n",
        "            do_rescale=False,\n",
        "            size={\"height\": IMAGE_HEIGHT, \"width\": IMAGE_WIDTH}\n",
        "        )\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = KITTIDataset(\n",
        "            TRAIN_DIR,\n",
        "            TRAIN_LABELS_DIR,\n",
        "            feature_extractor,\n",
        "            transforms=train_transforms,\n",
        "            use_reduced_classes=USE_REDUCED_CLASSES\n",
        "        )\n",
        "\n",
        "        val_dataset = KITTIDataset(\n",
        "            VAL_DIR,\n",
        "            VAL_LABELS_DIR,\n",
        "            feature_extractor,\n",
        "            transforms=val_transforms,\n",
        "            use_reduced_classes=USE_REDUCED_CLASSES\n",
        "        )\n",
        "\n",
        "        print(f\"Number of classes: {train_dataset.num_classes}\")\n",
        "        print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "        print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "            f\"nvidia/mit-{MODEL_TYPE}\",\n",
        "            num_labels=train_dataset.num_classes,\n",
        "            id2label={str(i): str(i) for i in range(train_dataset.num_classes)},\n",
        "            label2id={str(i): i for i in range(train_dataset.num_classes)},\n",
        "            ignore_mismatched_sizes=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Enable memory efficient attention if available\n",
        "        if hasattr(model.config, 'use_memory_efficient_attention'):\n",
        "            model.config.use_memory_efficient_attention = True\n",
        "\n",
        "        # For transfer learning experiment, load CamVid weights\n",
        "        if experiment_type == 'transfer' and os.path.exists(CAMVID_CHECKPOINT):\n",
        "            print(f\"Loading weights from CamVid checkpoint: {CAMVID_CHECKPOINT}\")\n",
        "            # Load state dictionary\n",
        "            checkpoint = torch.load(CAMVID_CHECKPOINT, map_location=device)\n",
        "\n",
        "            if 'model_state_dict' in checkpoint:\n",
        "                state_dict = checkpoint['model_state_dict']\n",
        "            else:\n",
        "                state_dict = checkpoint  # In case the entire state dict was saved\n",
        "\n",
        "            # Filter out mismatched keys (especially the classification head)\n",
        "            model_dict = model.state_dict()\n",
        "            pretrained_dict = {k: v for k, v in state_dict.items()\n",
        "                              if k in model_dict and v.shape == model_dict[k].shape}\n",
        "\n",
        "            # Update model with pretrained weights\n",
        "            model_dict.update(pretrained_dict)\n",
        "            model.load_state_dict(model_dict)\n",
        "\n",
        "            print(f\"Loaded {len(pretrained_dict)}/{len(model_dict)} layers from checkpoint\")\n",
        "\n",
        "            # Freeze encoder parameters if specified\n",
        "            if FREEZE_ENCODER:\n",
        "                print(\"Freezing encoder parameters...\")\n",
        "                for name, param in model.segformer.encoder.named_parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                # Only train the decoder (segmentation head)\n",
        "                trainable_params = sum(p.numel() for p in model.decode_head.parameters() if p.requires_grad)\n",
        "                total_params = sum(p.numel() for p in model.parameters())\n",
        "                print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
        "\n",
        "        # Initialize optimizer and scheduler\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=LEARNING_RATE,\n",
        "            weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "\n",
        "        total_steps = len(train_loader) * NUM_EPOCHS // ACCUMULATION_STEPS\n",
        "        scheduler = OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=LEARNING_RATE,\n",
        "            total_steps=total_steps,\n",
        "            pct_start=0.1\n",
        "        )\n",
        "\n",
        "        # Initialize criterion\n",
        "        criterion = EnhancedSegmentationLoss(train_dataset.num_classes).to(device)\n",
        "\n",
        "        # Initialize AMP scaler\n",
        "        scaler = torch.amp.GradScaler()\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        all_metrics = []\n",
        "        component_losses = {'train': [], 'val': []}\n",
        "        best_val_miou = 0\n",
        "\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "            print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "            # Training\n",
        "            train_loss, batch_losses, train_comp_losses = train_epoch(\n",
        "                model, train_loader, optimizer, scheduler, scaler, criterion, device, epoch\n",
        "            )\n",
        "            train_losses.append(train_loss)\n",
        "            component_losses['train'].append(train_comp_losses)\n",
        "\n",
        "            # Validation\n",
        "            val_metrics, predictions, ground_truths, image_names = validate(\n",
        "                model, val_loader, criterion, device, val_dataset\n",
        "            )\n",
        "            val_losses.append(val_metrics['val_loss'])\n",
        "            component_losses['val'].append(val_metrics['component_losses'])\n",
        "            all_metrics.append(val_metrics)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"Val Loss: {val_metrics['val_loss']:.4f}\")\n",
        "            print(f\"Val Mean IoU: {val_metrics['mean_iou']:.4f}\")\n",
        "            print(f\"Val Pixel Accuracy: {val_metrics['pixel_accuracy']:.4f}\")\n",
        "\n",
        "            # Generate visualizations for this epoch\n",
        "            epoch_vis_dir = os.path.join(exp_visualization_dir, f'epoch_{epoch+1}')\n",
        "            visualize_predictions(\n",
        "                image_names[:10],  # Use first 10 images\n",
        "                predictions[:10],\n",
        "                ground_truths[:10],\n",
        "                val_dataset.get_color_map(),\n",
        "                val_dataset.get_class_names(),\n",
        "                epoch_vis_dir\n",
        "            )\n",
        "\n",
        "            # Save checkpoint\n",
        "            if val_metrics['mean_iou'] > best_val_miou:\n",
        "                best_val_miou = val_metrics['mean_iou']\n",
        "                checkpoint = {\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'val_loss': val_metrics['val_loss'],\n",
        "                    'val_miou': val_metrics['mean_iou'],\n",
        "                    'config': {\n",
        "                        'model_type': MODEL_TYPE,\n",
        "                        'image_size': (IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "                        'num_classes': train_dataset.num_classes,\n",
        "                        'class_names': val_dataset.get_class_names(),\n",
        "                        'use_reduced_classes': USE_REDUCED_CLASSES,\n",
        "                        'transfer_learning': experiment_type == 'transfer',\n",
        "                        'freeze_encoder': FREEZE_ENCODER and experiment_type == 'transfer'\n",
        "                    }\n",
        "                }\n",
        "                checkpoint_path = os.path.join(\n",
        "                    exp_checkpoint_dir,\n",
        "                    f'best_model_miou_{val_metrics[\"mean_iou\"]:.4f}.pth'\n",
        "                )\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "                # Also save the best metrics for later comparison\n",
        "                with open(os.path.join(exp_metrics_dir, 'best_metrics.json'), 'w') as f:\n",
        "                    json.dump(val_metrics, f, indent=2)\n",
        "\n",
        "                print(f\"New best model saved! Val Mean IoU: {val_metrics['mean_iou']:.4f}\")\n",
        "\n",
        "            # Plot training curves\n",
        "            plot_training_curves(\n",
        "                train_losses,\n",
        "                val_losses,\n",
        "                component_losses,\n",
        "                {\n",
        "                    'mean_iou': [m['mean_iou'] for m in all_metrics],\n",
        "                    'pixel_accuracy': [m['pixel_accuracy'] for m in all_metrics],\n",
        "                    'freq_weighted_iou': [m['freq_weighted_iou'] for m in all_metrics],\n",
        "                    'class_iou': [m['class_iou'] for m in all_metrics]\n",
        "                },\n",
        "                exp_visualization_dir\n",
        "            )\n",
        "\n",
        "            # Save metrics for this epoch\n",
        "            with open(os.path.join(exp_metrics_dir, f'metrics_epoch_{epoch+1}.json'), 'w') as f:\n",
        "                json.dump(val_metrics, f, indent=2)\n",
        "\n",
        "            # Clear cache after each epoch\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"Training completed for {experiment_type} experiment!\")\n",
        "\n",
        "        # Save final model\n",
        "        final_checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'config': {\n",
        "                'model_type': MODEL_TYPE,\n",
        "                'image_size': (IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "                'num_classes': train_dataset.num_classes,\n",
        "                'class_names': val_dataset.get_class_names(),\n",
        "                'use_reduced_classes': USE_REDUCED_CLASSES,\n",
        "                'transfer_learning': experiment_type == 'transfer'\n",
        "            }\n",
        "        }\n",
        "        torch.save(\n",
        "            final_checkpoint,\n",
        "            os.path.join(exp_checkpoint_dir, f'final_model.pth')\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Functions for running experiments in Jupyter\n",
        "def run_baseline():\n",
        "    \"\"\"Run baseline experiment (train from scratch).\"\"\"\n",
        "    print(\"Running baseline experiment (SegFormer on KITTI from scratch)\")\n",
        "    train_and_evaluate('baseline')\n",
        "\n",
        "def run_transfer():\n",
        "    \"\"\"Run transfer learning experiment (using CamVid weights).\"\"\"\n",
        "    print(\"Running transfer learning experiment (CamVid  KITTI)\")\n",
        "    train_and_evaluate('transfer')\n",
        "\n",
        "def run_both():\n",
        "    \"\"\"Run both experiments sequentially and compare.\"\"\"\n",
        "    print(\"Running baseline experiment (SegFormer on KITTI from scratch)\")\n",
        "    train_and_evaluate('baseline')\n",
        "    print(\"\\n\\nRunning transfer learning experiment (CamVid  KITTI)\")\n",
        "    train_and_evaluate('transfer')\n",
        "    print(\"\\n\\nGenerating comparison reports\")\n",
        "    evaluate_transfer_learning_performance()\n",
        "\n",
        "def run_comparison():\n",
        "    \"\"\"Just compare existing results.\"\"\"\n",
        "    print(\"Generating comparison reports\")\n",
        "    evaluate_transfer_learning_performance()\n",
        "\n",
        "# If running as a script (not in Jupyter)\n",
        "# Comment out the entire main block\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) > 1:\n",
        "        mode = sys.argv[1]\n",
        "        if mode == 'baseline':\n",
        "            run_baseline()\n",
        "        elif mode == 'transfer':\n",
        "            run_transfer()\n",
        "        elif mode == 'compare':\n",
        "            run_comparison()\n",
        "        else:\n",
        "            run_both()\n",
        "    else:\n",
        "        run_both()\n",
        "'''\n",
        "\n",
        "# Add explicit call to run_transfer at the bottom\n",
        "print(\"About to start transfer learning...\")\n",
        "run_comparison()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1c0be656",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def get_transform(image_height, image_width):\n",
        "    return A.Compose([\n",
        "        A.Resize(height=image_height, width=image_width),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def register_hooks(model, activations, gradients):\n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output)\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "\n",
        "    handle_f = model.decode_head.classifier.register_forward_hook(forward_hook)\n",
        "    handle_b = model.decode_head.classifier.register_backward_hook(backward_hook)\n",
        "    return handle_f, handle_b\n",
        "\n",
        "def generate_gradcam(model, input_tensor, class_index, activations, gradients):\n",
        "    output = model(pixel_values=input_tensor)\n",
        "    logits = output.logits  # (1, C, H, W)\n",
        "\n",
        "    model.zero_grad()\n",
        "    logits[0, class_index].mean().backward()\n",
        "\n",
        "    act = activations[0].detach().cpu()[0]  # [C, H, W]\n",
        "    grad = gradients[0].detach().cpu()[0]   # [C, H, W]\n",
        "    weights = grad.mean(dim=(1, 2))         # GAP\n",
        "\n",
        "    cam = (weights[:, None, None] * act).sum(dim=0)\n",
        "    cam = torch.relu(cam)\n",
        "    cam -= cam.min()\n",
        "    cam /= cam.max()\n",
        "    return cam.numpy()\n",
        "\n",
        "def overlay_heatmap(original_img, cam):\n",
        "    cam_resized = cv2.resize(cam, (original_img.shape[1], original_img.shape[0]))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(original_img, 0.5, heatmap, 0.5, 0)\n",
        "    return overlay\n",
        "\n",
        "def run_gradcam(image_path, checkpoint_path, model_type='b3', image_height=1024, image_width=1024, class_index=None, output_path='gradcam_output.png'):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load model config\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        f\"nvidia/mit-{model_type}\",\n",
        "        num_labels=checkpoint['config']['num_classes'],\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])  # Load your trained weights\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Load and transform image\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    transform = get_transform(image_height, image_width)\n",
        "    transformed = transform(image=img_rgb)\n",
        "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Register hooks\n",
        "    activations, gradients = [], []\n",
        "    handle_f, handle_b = register_hooks(model, activations, gradients)\n",
        "\n",
        "    # Run forward pass and Grad-CAM\n",
        "    with torch.no_grad():\n",
        "        logits = model(pixel_values=input_tensor).logits\n",
        "\n",
        "    # Safe spatial indexing\n",
        "    _, _, H_out, W_out = logits.shape\n",
        "    class_index = logits[0, :, H_out // 2, W_out // 2].argmax().item()\n",
        "\n",
        "    cam = generate_gradcam(model, input_tensor, class_index, activations, gradients)\n",
        "\n",
        "    # Generate and save visualization\n",
        "    overlay = overlay_heatmap(img, cam)\n",
        "    cv2.imwrite(output_path, overlay)\n",
        "\n",
        "    # Cleanup\n",
        "    handle_f.remove()\n",
        "    handle_b.remove()\n",
        "\n",
        "    print(f\"Grad-CAM saved to: {output_path}\")\n",
        "\n",
        "# Example usage:\n",
        "# run_gradcam(\n",
        "#     image_path=\"PATH/TO/YOUR/TEST_IMAGE.png\",\n",
        "#     checkpoint_path=\"PATH/TO/best_model_miou.pth\",\n",
        "#     output_path=\"PATH/TO/save/gradcam.png\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "693bd614",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_multiclass_gradcam(image_path, checkpoint_path, model_type='b3', image_height=1024, image_width=1024, output_dir='gradcam_outputs'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    num_classes = checkpoint['config']['num_classes']\n",
        "\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        f\"nvidia/mit-{model_type}\",\n",
        "        num_labels=num_classes,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device).eval()\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    transform = get_transform(image_height, image_width)\n",
        "    transformed = transform(image=img_rgb)\n",
        "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    activations, gradients = [], []\n",
        "    handle_f, handle_b = register_hooks(model, activations, gradients)\n",
        "\n",
        "    # Run a forward pass once\n",
        "    with torch.no_grad():\n",
        "        logits = model(pixel_values=input_tensor).logits\n",
        "\n",
        "    for class_index in range(num_classes):\n",
        "        # Reset hooks storage for clean gradients\n",
        "        activations.clear()\n",
        "        gradients.clear()\n",
        "\n",
        "        cam = generate_gradcam(model, input_tensor, class_index, activations, gradients)\n",
        "        overlay = overlay_heatmap(img, cam)\n",
        "\n",
        "        class_names = checkpoint['config'].get('class_names', [f\"Class_{i}\" for i in range(num_classes)])\n",
        "        out_path = os.path.join(output_dir, f\"{class_names[class_index]}_gradcam.png\")\n",
        "\n",
        "        cv2.imwrite(out_path, overlay)\n",
        "        print(f\"Saved: {out_path}\")\n",
        "\n",
        "    handle_f.remove()\n",
        "    handle_b.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6afe877a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "c:\\Users\\MSI GF66\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: output/gradcam_visualization.png\\background_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\road_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\sidewalk_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\building_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\wall_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\fence_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\pole_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\traffic light_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\traffic sign_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\vegetation_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\terrain_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\sky_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\person_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\rider_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\car_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\truck_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\bus_gradcam.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_32388\\1037711552.py:47: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: output/gradcam_visualization.png\\motorcycle_gradcam.png\n",
            "Saved: output/gradcam_visualization.png\\bicycle_gradcam.png\n"
          ]
        }
      ],
      "source": [
        "run_multiclass_gradcam(\n",
        "    image_path=\"KITTI/testing/image_2/000135_10.png\",\n",
        "    checkpoint_path=\"best_model_miou_0.5342.pth\",\n",
        "    output_dir=\"output/gradcam_visualization.png\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6d44fb37",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from transformers import SegformerForSemanticSegmentation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def get_transform(image_height, image_width):\n",
        "    return A.Compose([\n",
        "        A.Resize(height=image_height, width=image_width),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def register_hooks(model, activations, gradients):\n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output)\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "\n",
        "    handle_f = model.decode_head.classifier.register_forward_hook(forward_hook)\n",
        "    handle_b = model.decode_head.classifier.register_backward_hook(backward_hook)\n",
        "    return handle_f, handle_b\n",
        "\n",
        "def generate_gradcam(model, input_tensor, class_index, activations, gradients):\n",
        "    output = model(pixel_values=input_tensor)\n",
        "    logits = output.logits\n",
        "\n",
        "    model.zero_grad()\n",
        "    logits[0, class_index].mean().backward()\n",
        "\n",
        "    act = activations[0].detach().cpu()[0]\n",
        "    grad = gradients[0].detach().cpu()[0]\n",
        "    weights = grad.mean(dim=(1, 2))\n",
        "\n",
        "    cam = (weights[:, None, None] * act).sum(dim=0)\n",
        "    cam = torch.relu(cam)\n",
        "    cam -= cam.min()\n",
        "    cam /= (cam.max() + 1e-5)\n",
        "    return cam.numpy()\n",
        "\n",
        "def apply_class_colormap(cam, color_map_id):\n",
        "    cam_uint8 = np.uint8(255 * cam)\n",
        "    heatmap = cv2.applyColorMap(cam_uint8, color_map_id)\n",
        "    return heatmap\n",
        "\n",
        "def create_label_legend(class_names, colormaps, save_path):\n",
        "    num_classes = len(class_names)\n",
        "    img_height = 30 * num_classes + 10\n",
        "    legend_img = Image.new(\"RGB\", (300, img_height), color=(255, 255, 255))\n",
        "    draw = ImageDraw.Draw(legend_img)\n",
        "    \n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", size=18)\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        color_rgb = tuple(int(255 * c) for c in plt.cm.get_cmap('tab20')(i % 20)[:3])\n",
        "        draw.rectangle([(10, i * 30 + 10), (30, i * 30 + 30)], fill=color_rgb)\n",
        "        draw.text((40, i * 30 + 10), name, fill=(0, 0, 0), font=font)\n",
        "\n",
        "    legend_img.save(save_path)\n",
        "    print(f\" Label legend saved at: {save_path}\")\n",
        "\n",
        "def run_combined_multiclass_gradcam(image_path, checkpoint_path, model_type='b3', image_height=1024, image_width=1024, output_path='output/combined_gradcam.png'):\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    num_classes = checkpoint['config']['num_classes']\n",
        "    class_names = checkpoint['config'].get('class_names', [f\"Class_{i}\" for i in range(num_classes)])\n",
        "\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        f\"nvidia/mit-{model_type}\",\n",
        "        num_labels=num_classes,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device).eval()\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    transform = get_transform(image_height, image_width)\n",
        "    transformed = transform(image=img_rgb)\n",
        "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    activations, gradients = [], []\n",
        "    handle_f, handle_b = register_hooks(model, activations, gradients)\n",
        "\n",
        "    colormaps = [\n",
        "        cv2.COLORMAP_JET, cv2.COLORMAP_HOT, cv2.COLORMAP_COOL, cv2.COLORMAP_OCEAN,\n",
        "        cv2.COLORMAP_SUMMER, cv2.COLORMAP_AUTUMN, cv2.COLORMAP_WINTER, cv2.COLORMAP_SPRING,\n",
        "        cv2.COLORMAP_PINK, cv2.COLORMAP_BONE\n",
        "    ]\n",
        "\n",
        "    combined_heatmap = np.zeros_like(img_rgb, dtype=np.float32)\n",
        "\n",
        "    for class_index in range(num_classes):\n",
        "        activations.clear()\n",
        "        gradients.clear()\n",
        "\n",
        "        cam = generate_gradcam(model, input_tensor, class_index, activations, gradients)\n",
        "        cam_resized = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
        "        heatmap = apply_class_colormap(cam_resized, colormaps[class_index % len(colormaps)])\n",
        "        heatmap_rgb = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "        # Boost and accumulate for final composite\n",
        "        combined_heatmap += 1.5 * heatmap_rgb\n",
        "\n",
        "        #  Save per-class heatmap\n",
        "        class_overlay = (0.5 * img_rgb / 255.0 + 0.5 * heatmap_rgb)\n",
        "        class_overlay = np.clip(class_overlay * 255, 0, 255).astype(np.uint8)\n",
        "        class_name = class_names[class_index].replace(\" \", \"_\").lower()\n",
        "        class_out_path = os.path.join(os.path.dirname(output_path), f\"gradcam_{class_name}.png\")\n",
        "        cv2.imwrite(class_out_path, cv2.cvtColor(class_overlay, cv2.COLOR_RGB2BGR))\n",
        "        print(f\" Saved individual Grad-CAM for: {class_name}\")\n",
        "\n",
        "\n",
        "    combined_heatmap = np.clip(combined_heatmap, 0, 1)\n",
        "    overlay = (0.5 * img_rgb / 255.0 + 0.5 * combined_heatmap)\n",
        "    overlay = np.clip(overlay * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    cv2.imwrite(output_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
        "    print(f\" Combined Grad-CAM saved at: {output_path}\")\n",
        "\n",
        "    label_path = output_path.replace(\".png\", \"_labels.png\")\n",
        "    create_label_legend(class_names, colormaps, label_path)\n",
        "\n",
        "    handle_f.remove()\n",
        "    handle_b.remove()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# run_combined_multiclass_gradcam(\n",
        "#     image_path=\"KITTI/testing/image_2/000089_10.png\",\n",
        "#     checkpoint_path=\"best_model_miou_0.5342.pth\",\n",
        "#     output_path=\"output/combined_gradcam.png\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0a841fe4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Saved individual Grad-CAM for: background\n",
            " Saved individual Grad-CAM for: road\n",
            " Saved individual Grad-CAM for: sidewalk\n",
            " Saved individual Grad-CAM for: building\n",
            " Saved individual Grad-CAM for: wall\n",
            " Saved individual Grad-CAM for: fence\n",
            " Saved individual Grad-CAM for: pole\n",
            " Saved individual Grad-CAM for: traffic_light\n",
            " Saved individual Grad-CAM for: traffic_sign\n",
            " Saved individual Grad-CAM for: vegetation\n",
            " Saved individual Grad-CAM for: terrain\n",
            " Saved individual Grad-CAM for: sky\n",
            " Saved individual Grad-CAM for: person\n",
            " Saved individual Grad-CAM for: rider\n",
            " Saved individual Grad-CAM for: car\n",
            " Saved individual Grad-CAM for: truck\n",
            " Saved individual Grad-CAM for: bus\n",
            " Saved individual Grad-CAM for: motorcycle\n",
            " Saved individual Grad-CAM for: bicycle\n",
            " Combined Grad-CAM saved at: output/gradcam_visualization_combined.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_6592\\3156948469.py:63: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  color_rgb = tuple(int(255 * c) for c in plt.cm.get_cmap('tab20')(i % 20)[:3])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Label legend saved at: output/gradcam_visualization_combined_labels.png\n"
          ]
        }
      ],
      "source": [
        "run_combined_multiclass_gradcam(\n",
        "    image_path=\"KITTI/testing/image_2/000089_10.png\",\n",
        "    checkpoint_path=\"best_model_miou_0.5342.pth\",\n",
        "    output_path=\"output/gradcam_visualization_combined.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8563faf9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\MSI GF66\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import SegformerForSemanticSegmentation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Resize and normalize transforms\n",
        "def get_transform(image_height, image_width):\n",
        "    return A.Compose([\n",
        "        A.Resize(height=image_height, width=image_width),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "# Extract attention rollout from MiT encoder\n",
        "def get_attention_rollout(model, input_tensor, image_size):\n",
        "    attention_maps = []\n",
        "\n",
        "    def hook(module, input, output):\n",
        "        attention = module.attention_scores.detach().cpu()\n",
        "        attention_maps.append(attention)\n",
        "\n",
        "    handles = []\n",
        "    for blk in model.segformer.encoder.block:\n",
        "        handles.append(blk.attn.register_forward_hook(hook))\n",
        "\n",
        "    _ = model(pixel_values=input_tensor)\n",
        "\n",
        "    for h in handles:\n",
        "        h.remove()\n",
        "\n",
        "    # Stack attention maps and average heads: [layers, heads, N, N] -> [layers, N, N]\n",
        "    attn = torch.stack(attention_maps)[:, 0]  # remove batch dim\n",
        "    attn = attn.mean(dim=1)\n",
        "\n",
        "    # Add identity and normalize\n",
        "    eye = torch.eye(attn.size(-1)).to(attn.device)\n",
        "    attn = attn + eye\n",
        "    attn = attn / attn.sum(dim=-1, keepdim=True)\n",
        "\n",
        "    rollout = attn[0]\n",
        "    for i in range(1, attn.shape[0]):\n",
        "        rollout = attn[i] @ rollout\n",
        "\n",
        "    cls_attn = rollout[0, 1:]  # exclude CLS token\n",
        "    num_patches = cls_attn.shape[0]\n",
        "    h = w = int(num_patches ** 0.5)\n",
        "    cls_attn_map = cls_attn.reshape(h, w)\n",
        "\n",
        "    # Resize to full image size\n",
        "    cls_attn_map = cv2.resize(cls_attn_map.numpy(), image_size, interpolation=cv2.INTER_LINEAR)\n",
        "    cls_attn_map = (cls_attn_map - cls_attn_map.min()) / (cls_attn_map.max() - cls_attn_map.min())\n",
        "    return cls_attn_map\n",
        "\n",
        "# Top-level function: load model + image + save overlay\n",
        "def visualize_rollout_on_image(image_path, checkpoint_path, model_type=\"b3\",\n",
        "                               image_height=1024, image_width=1024,\n",
        "                               output_path=\"attention_rollout.png\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # Load model and restore trained weights\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        f\"nvidia/mit-{model_type}\",\n",
        "        num_labels=checkpoint['config']['num_classes'],\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img_bgr = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    transform = get_transform(image_height, image_width)\n",
        "    transformed = transform(image=img_rgb)\n",
        "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Compute attention rollout\n",
        "    rollout_map = get_attention_rollout(model, input_tensor, (image_width, image_height))\n",
        "\n",
        "    # Create overlay\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * rollout_map), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    overlay = cv2.addWeighted(img_rgb, 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    # Save\n",
        "    cv2.imwrite(output_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
        "    print(f\" Saved attention rollout to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0fec7350",
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'checkpoints/transfer/best_model_miou.pth'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_rollout_on_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKITTI/testing/image_2/000093_10.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/transfer/best_model_miou.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/attention_rollout.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[2], line 62\u001b[0m, in \u001b[0;36mvisualize_rollout_on_image\u001b[1;34m(image_path, checkpoint_path, model_type, image_height, image_width, output_path)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisualize_rollout_on_image\u001b[39m(image_path, checkpoint_path, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     59\u001b[0m                                image_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, image_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m     60\u001b[0m                                output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_rollout.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     61\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Load model and restore trained weights\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     model \u001b[38;5;241m=\u001b[39m SegformerForSemanticSegmentation\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia/mit-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         num_labels\u001b[38;5;241m=\u001b[39mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     68\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\MSI GF66\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:1479\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[1;32mc:\\Users\\MSI GF66\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[1;32mc:\\Users\\MSI GF66\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/transfer/best_model_miou.pth'"
          ]
        }
      ],
      "source": [
        "visualize_rollout_on_image(\n",
        "    image_path=\"KITTI/testing/image_2/000093_10.png\",\n",
        "    checkpoint_path=\"checkpoints/transfer/best_model_miou.pth\",\n",
        "    output_path=\"output/attention_rollout.png\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ddc3f58",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
